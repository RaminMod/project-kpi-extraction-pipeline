{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa87ea98-482b-4dea-80d0-5ba86cfa4618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Installing required libraries\n",
    "\n",
    "In this cell I'm just preparing the environment for the whole extraction pipeline.  \n",
    "I upgrade `pip` and install the libraries I need to read PowerPoint, Word, and Excel files, work with tables using pandas, and call the OpenAI model.  \n",
    "Nothing complicated here — just making sure everything is ready for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc45795f-294a-4f37-b952-397c5ccbe3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (25.3)\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: openai==1.55.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (1.55.3)\n",
      "Requirement already satisfied: httpx==0.27.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: python-pptx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: python-docx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.10/site-packages (from openai==1.55.3) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.55.3) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openai==1.55.3) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.10/site-packages (from openai==1.55.3) (1.10.6)\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from openai==1.55.3) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openai==1.55.3) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openai==1.55.3) (4.15.0)\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx==0.27.2) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from httpx==0.27.2) (1.0.9)\n",
      "Requirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from httpx==0.27.2) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /databricks/python3/lib/python3.10/site-packages (from python-pptx) (9.4.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from python-pptx) (3.2.9)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /databricks/python3/lib/python3.10/site-packages (from python-pptx) (4.9.1)\n",
      "Requirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install required packages\n",
    "%pip install --upgrade pip\n",
    "%pip install openai==1.55.3 httpx==0.27.2 python-pptx python-docx openpyxl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1439687b-48c3-4928-a65c-597e9a4702ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70958f55-7caf-4a6c-a6fb-3a29dfe909c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) Initializing the Azure OpenAI client\n",
    "\n",
    "This cell sets up the Azure OpenAI configuration by defining the endpoint, API key, and client object used for sending requests to the GPT model.  \n",
    "These settings allow the notebook to communicate with the Azure OpenAI deployment throughout the extraction and analysis workflow.  \n",
    "For production use, the API key should be stored securely in a Databricks secret scope rather than directly in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7ecd75-229c-46c4-ac77-33efcaa3223c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Read secrets from Databricks Secret Scope\n",
    "AZURE_OPENAI_ENDPOINT = dbutils.secrets.get(\"itera-secrets\", \"AZURE-OPENAI-ENDPOINT\")\n",
    "AZURE_OPENAI_KEY = dbutils.secrets.get(\"itera-secrets\", \"AZURE-OPENAI-KEY\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91321e69-065d-467e-a693-7206968ab0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuring SAS access for Azure Data Lake (ABFS)\n",
    "\n",
    "This cell sets up Spark to authenticate against Azure Data Lake Gen2 using a SAS token.  \n",
    "The configuration enables the notebook to read and process files stored in the specified container by registering the SAS token with the ABFS driver.  \n",
    "A base `abfss://` path is also built here, which is reused in the following cells when loading documents from storage.  \n",
    "For long-term or production use, the SAS token should be stored securely (for example, in a Databricks secret scope) instead of being written directly in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3968b7d3-6a1e-421f-91e9-1c7f7e6b4578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read SAS token securely from Databricks secret scope\n",
    "SAS_TOKEN = dbutils.secrets.get(\"itera-secrets\", \"ADLS-SAS-TOKEN\")\n",
    "ACCOUNT   = \"iterastorerm\"\n",
    "CONTAINER = \"itera-extraction-project\"\n",
    "\n",
    "# Configure Spark to use the SAS for ABFS (Azure Data Lake Gen2)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{ACCOUNT}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.token.provider.type.{ACCOUNT}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "# lstrip('?') removes the leading '?' so Spark gets only the token body\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.fixed.token.{ACCOUNT}.dfs.core.windows.net\",\n",
    "    SAS_TOKEN.lstrip(\"?\")\n",
    ")\n",
    "\n",
    "# Build the base ABFSS path used in the next cells\n",
    "BASE_ABFSS = f\"abfss://{CONTAINER}@{ACCOUNT}.dfs.core.windows.net\"\n",
    "INPUT_DIR  = BASE_ABFSS\n",
    "print(\"Configured. Base path:\", BASE_ABFSS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b4004e-43b6-45e5-a4ac-727b8b644fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/TG3 Project Plan + Establishment v1.2.pptx</td><td>TG3 Project Plan + Establishment v1.2.pptx</td><td>535197</td><td>1764943162000</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/all_content.txt</td><td>all_content.txt</td><td>23812</td><td>1764945519000</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/output.json</td><td>output.json</td><td>1415</td><td>1764945524000</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/practice/</td><td>practice/</td><td>0</td><td>0</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/resources_kpi.json</td><td>resources_kpi.json</td><td>20</td><td>1763563201000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/TG3 Project Plan + Establishment v1.2.pptx",
         "TG3 Project Plan + Establishment v1.2.pptx",
         535197,
         1764943162000
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/all_content.txt",
         "all_content.txt",
         23812,
         1764945519000
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/output.json",
         "output.json",
         1415,
         1764945524000
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/practice/",
         "practice/",
         0,
         0
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/resources_kpi.json",
         "resources_kpi.json",
         20,
         1763563201000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing files in the storage container\n",
    "display(dbutils.fs.ls(INPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a1eff1-8c5f-4453-aab2-7a3210095289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Defining helper functions for reading DOCX, XLSX, and PPTX files\n",
    "\n",
    "In the next few cells, helper functions are defined for extracting content from Word, Excel, and PowerPoint files stored in Azure Data Lake.\n",
    "\n",
    "These functions together make it possible to:\n",
    "- Load binary files directly from ABFS using Spark.\n",
    "- Read text and tables from Word (.docx) files.\n",
    "- Read data from all sheets inside Excel (.xlsx) files.\n",
    "- Parse PowerPoint (.pptx) slides and extract slide text, native tables, slide notes, and detect embedded Excel objects.\n",
    "- When embedded Excel workbooks exist inside a slide, extract them and place the content directly under the slide where they appear, so the structure of the presentation is kept.\n",
    "\n",
    "Even though the functions are split across separate cells, they work together as one toolbox and form the core of the data-extraction pipeline, allowing the notebook to turn different document formats into structured, readable output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19b60ab-f2c6-4255-9953-52dfabd37b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing all required libraries for reading, parsing, and processing DOCX, XLSX, and PPTX files.\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "from docx import Document\n",
    "from openpyxl import load_workbook\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3325d99-2299-4dce-8d20-821b5a82ab3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AZURE DATA LAKE FILE READER\n",
    "def _read_bytes_abfs(path):\n",
    "    \"\"\"\n",
    "    Read a file from Azure Blob File System (ABFS) and return its raw bytes.\n",
    "    \n",
    "    Args:\n",
    "        path (str): ABFS path to the file\n",
    "        \n",
    "    Returns:\n",
    "        bytes: Raw file content as bytes\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist at the given path\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"binaryFile\").load(path)\n",
    "    rows = df.collect()\n",
    "    \n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    \n",
    "    return rows[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb186618-652a-4ef5-9695-15b8627f1c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WORD DOCUMENT (.docx) EXTRACTION\n",
    "def read_docx_text(abfs_path, include_tables=False):\n",
    "    \"\"\"\n",
    "    Extract all text content from a Word document, optionally including tables.\n",
    "    \n",
    "    Args:\n",
    "        abfs_path (str): Path to the .docx file in Azure Data Lake\n",
    "        include_tables (bool): If True, extracts table content as well\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text with paragraphs and tables (if requested)\n",
    "        \n",
    "    Example:\n",
    "        >>> text = read_docx_text(\"abfs://container/document.docx\", include_tables=True)\n",
    "        >>> print(text)\n",
    "    \"\"\"\n",
    "    # Load the Word document from Azure\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "    doc = Document(io.BytesIO(data))\n",
    "    \n",
    "    content = []\n",
    "    \n",
    "    # Extract all paragraphs\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        if text:  # Only include non-empty paragraphs\n",
    "            content.append(text)\n",
    "    \n",
    "    # Extract tables if requested\n",
    "    if include_tables:\n",
    "        for i, table in enumerate(doc.tables):\n",
    "            content.append(f\"\\n--- Table {i + 1} ---\")\n",
    "            \n",
    "            for row in table.rows:\n",
    "                # Get text from each cell in the row\n",
    "                cells = [cell.text.strip() for cell in row.cells]\n",
    "                \n",
    "                # Only include rows that have some content\n",
    "                if any(cells):\n",
    "                    content.append(\" | \".join(cells))\n",
    "    \n",
    "    return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e966ad-3aa1-4039-a14e-784065acae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EXCEL WORKBOOK (.xlsx) EXTRACTION\n",
    "def read_xlsx_sheets(abfs_path):\n",
    "    \"\"\"\n",
    "    Read all sheets from an Excel workbook into pandas DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        abfs_path (str): Path to the .xlsx file in Azure Data Lake\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with sheet names as keys and DataFrames as values\n",
    "              Example: {'Sheet1': DataFrame, 'Sales Data': DataFrame}\n",
    "              \n",
    "    Example:\n",
    "        >>> sheets = read_xlsx_sheets(\"abfs://container/data.xlsx\")\n",
    "        >>> for sheet_name, df in sheets.items():\n",
    "        ...     print(f\"{sheet_name}: {len(df)} rows\")\n",
    "    \"\"\"\n",
    "    # Load the Excel file from Azure\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "    workbook = load_workbook(io.BytesIO(data), data_only=True)\n",
    "\n",
    "    sheets = {}\n",
    "    \n",
    "    for sheet in workbook.worksheets:\n",
    "        # Get all cell values from the sheet\n",
    "        rows = list(sheet.values)\n",
    "        \n",
    "        if not rows:\n",
    "            continue  # Skip empty sheets\n",
    "        \n",
    "        # First row is assumed to be the header\n",
    "        header = rows[0]\n",
    "        data_rows = rows[1:]\n",
    "        \n",
    "        # Handle cases where headers are missing or all None\n",
    "        if not header or all(h is None for h in header):\n",
    "            num_cols = len(data_rows[0]) if data_rows else 0\n",
    "            header = [f\"Column_{i+1}\" for i in range(num_cols)]\n",
    "        \n",
    "        # Create DataFrame from the data\n",
    "        df = pd.DataFrame(data_rows, columns=header)\n",
    "        \n",
    "        # Remove completely empty rows (all NaN values)\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        sheets[sheet.title] = df\n",
    "    \n",
    "    return sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fa5d93-2d0d-4114-b0dd-24942aa23a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# POWERPOINT EMBEDDED EXCEL EXTRACTION (STANDALONE)\n",
    "def read_pptx_embedded_excels(abfs_path):\n",
    "    \"\"\"\n",
    "    Extract all embedded Excel workbooks from a PowerPoint file.\n",
    "    \n",
    "    This function is useful when you only want the Excel data without\n",
    "    the PowerPoint text content.\n",
    "    \n",
    "    Args:\n",
    "        abfs_path (str): Path to the .pptx file in Azure Data Lake\n",
    "        \n",
    "    Returns:\n",
    "        dict: Nested dictionary structure:\n",
    "              {\n",
    "                  'ppt/embeddings/embeddedWorkbook1.xlsx': {\n",
    "                      'Sheet1': DataFrame,\n",
    "                      'Sheet2': DataFrame\n",
    "                  },\n",
    "                  'ppt/embeddings/embeddedWorkbook2.xlsx': {\n",
    "                      'Data': DataFrame\n",
    "                  }\n",
    "              }\n",
    "              \n",
    "    Example:\n",
    "        >>> embedded = read_pptx_embedded_excels(\"abfs://container/presentation.pptx\")\n",
    "        >>> for file_path, sheets in embedded.items():\n",
    "        ...     print(f\"Found: {file_path}\")\n",
    "    \"\"\"\n",
    "    # Load the PowerPoint file from Azure\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "    embedded = {}\n",
    "\n",
    "    # PowerPoint files are actually ZIP archives - open as such\n",
    "    with zipfile.ZipFile(io.BytesIO(data)) as z:\n",
    "        # Find all embedded Excel files in the ZIP structure\n",
    "        excel_files = [\n",
    "            name for name in z.namelist()\n",
    "            if name.startswith(\"ppt/embeddings/\")\n",
    "            and (name.endswith(\".xlsx\") or name.endswith(\".xlsm\"))\n",
    "        ]\n",
    "\n",
    "        # Process each embedded Excel file\n",
    "        for name in excel_files:\n",
    "            wb_bytes = z.read(name)\n",
    "            wb = load_workbook(io.BytesIO(wb_bytes), data_only=True)\n",
    "\n",
    "            sheet_dfs = {}\n",
    "            \n",
    "            for sheet in wb.worksheets:\n",
    "                rows = list(sheet.values)\n",
    "                \n",
    "                if not rows:\n",
    "                    continue  # Skip empty sheets\n",
    "                \n",
    "                header = rows[0]\n",
    "                data_rows = rows[1:]\n",
    "\n",
    "                # Generate generic headers if missing\n",
    "                if not header or all(h is None for h in header):\n",
    "                    num_cols = len(data_rows[0]) if data_rows else 0\n",
    "                    header = [f\"Column_{i+1}\" for i in range(num_cols)]\n",
    "\n",
    "                df = pd.DataFrame(data_rows, columns=header)\n",
    "                df = df.dropna(how=\"all\")\n",
    "                sheet_dfs[sheet.title] = df\n",
    "\n",
    "            # Only include workbooks that have at least one non-empty sheet\n",
    "            if sheet_dfs:\n",
    "                embedded[name] = sheet_dfs\n",
    "\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d9e7fb-1b70-4911-b759-0903b807c07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_smartart_texts(slide):\n",
    "    \"\"\"\n",
    "    Extract raw SmartArt texts (Sponsor, Project Manager, names, etc.)\n",
    "    from the SmartArt XML parts of a slide.\n",
    "    Returns a flat list of strings in the same order as in the SmartArt pane.\n",
    "    \"\"\"\n",
    "    smartart_texts = []\n",
    "    slide_part = slide.part\n",
    "\n",
    "    for rel in slide_part.rels.values():\n",
    "        if \"diagram\" in rel.reltype:  # SmartArt / diagram parts\n",
    "            try:\n",
    "                part = rel.target_part\n",
    "                root = ET.fromstring(part.blob)\n",
    "                for el in root.iter():\n",
    "                    # SmartArt text nodes are typically <dgm:t>\n",
    "                    if el.tag.endswith(\"}t\") and el.text and el.text.strip():\n",
    "                        smartart_texts.append(el.text.strip())\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return smartart_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b419f0-d331-4c80-9b11-b5c7d4db1bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_pptx_text(abfs_path, include_embedded_excels=True, max_rows=10):\n",
    "    \"\"\"\n",
    "    Read text from a PowerPoint presentation.\n",
    "    - Extracts slide text (including grouped shapes), native tables, notes.\n",
    "    - Extracts embedded Excel workbooks and attaches them under the slide.\n",
    "    - Extracts SmartArt / diagram text per slide and outputs JSON-structured blocks.\n",
    "    \"\"\"\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "\n",
    "    # --------- Step 1: Load all embedded Excel workbooks into a cache ---------\n",
    "    embedded_cache = {}\n",
    "    if include_embedded_excels:\n",
    "        with zipfile.ZipFile(io.BytesIO(data)) as z:\n",
    "            excel_files = [\n",
    "                name for name in z.namelist()\n",
    "                if name.startswith(\"ppt/embeddings/\")\n",
    "                and (name.endswith(\".xlsx\") or name.endswith(\".xlsm\"))\n",
    "            ]\n",
    "            \n",
    "            for name in excel_files:\n",
    "                wb_bytes = z.read(name)\n",
    "                wb = load_workbook(io.BytesIO(wb_bytes), data_only=True)\n",
    "\n",
    "                sheet_dfs = {}\n",
    "                for sheet in wb.worksheets:\n",
    "                    rows = list(sheet.values)\n",
    "                    if not rows:\n",
    "                        continue\n",
    "\n",
    "                    header = rows[0]\n",
    "                    data_rows = rows[1:]\n",
    "\n",
    "                    if not header or all(h is None for h in header):\n",
    "                        num_cols = len(data_rows[0]) if data_rows else 0\n",
    "                        header = [f\"Column_{i+1}\" for i in range(num_cols)]\n",
    "\n",
    "                    df = pd.DataFrame(data_rows, columns=header)\n",
    "                    df = df.dropna(how=\"all\")\n",
    "                    sheet_dfs[sheet.title] = df\n",
    "\n",
    "                if sheet_dfs:\n",
    "                    # Store by filename for lookup\n",
    "                    embedded_cache[name] = sheet_dfs\n",
    "\n",
    "    # --------- Step 2: Parse slides and match embedded files via relationships ---------\n",
    "    prs = Presentation(io.BytesIO(data))\n",
    "    content = []\n",
    "\n",
    "    for slide_num, slide in enumerate(prs.slides, start=1):\n",
    "        content.append(f\"\\n--- Slide {slide_num} ---\")\n",
    "        slide_has_content = False\n",
    "\n",
    "        # Helper to extract embedded file path from shape relationships\n",
    "        def get_embedded_excel_path(shape, slide):\n",
    "            \"\"\"Get the path to embedded Excel file for this specific shape\"\"\"\n",
    "            try:\n",
    "                # Check if shape has an OLE object\n",
    "                if \"oleObj\" not in shape._element.xml:\n",
    "                    return None\n",
    "                \n",
    "                # Parse the shape XML to find the relationship ID\n",
    "                shape_xml = shape._element.xml\n",
    "                \n",
    "                # Look for r:id or r:embed attributes\n",
    "                rid_match = re.search(r'r:id=\"(rId\\d+)\"', shape_xml)\n",
    "                if not rid_match:\n",
    "                    rid_match = re.search(r'r:embed=\"(rId\\d+)\"', shape_xml)\n",
    "                \n",
    "                if not rid_match:\n",
    "                    return None\n",
    "                \n",
    "                rid = rid_match.group(1)\n",
    "                \n",
    "                # Get the actual file path from slide relationships\n",
    "                slide_part = slide.part\n",
    "                if rid in slide_part.rels:\n",
    "                    rel = slide_part.rels[rid]\n",
    "                    target = rel.target_ref\n",
    "                    \n",
    "                    # Convert relative path to full path\n",
    "                    if target.startswith(\"../embeddings/\"):\n",
    "                        return f\"ppt/embeddings/{target.split('/')[-1]}\"\n",
    "                    elif target.startswith(\"embeddings/\"):\n",
    "                        return f\"ppt/{target}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "        # Helper to handle normal shapes + grouped shapes\n",
    "        def extract_from_shape(shape):\n",
    "            nonlocal slide_has_content\n",
    "\n",
    "            # If this is a group shape, recurse into its children\n",
    "            if hasattr(shape, \"shapes\") and len(shape.shapes) > 0:\n",
    "                for sub in shape.shapes:\n",
    "                    extract_from_shape(sub)\n",
    "                return\n",
    "\n",
    "            # 1) Standard text frame\n",
    "            if hasattr(shape, \"text_frame\"):\n",
    "                text = shape.text_frame.text.strip()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "                    slide_has_content = True\n",
    "\n",
    "            # 2) Simple text attribute\n",
    "            elif hasattr(shape, \"text\"):\n",
    "                text = shape.text.strip()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "                    slide_has_content = True\n",
    "\n",
    "            # 3) Native PPT tables\n",
    "            if hasattr(shape, \"has_table\") and shape.has_table:\n",
    "                content.append(f\"\\n[Table in Slide {slide_num}]\")\n",
    "                table = shape.table\n",
    "                for row in table.rows:\n",
    "                    cells = [cell.text.strip() for cell in row.cells]\n",
    "                    if any(cells):\n",
    "                        content.append(\" | \".join(cells))\n",
    "                slide_has_content = True\n",
    "\n",
    "            # 4) Embedded Excel / OLE objects: get the CORRECT workbook for THIS shape\n",
    "            if include_embedded_excels:\n",
    "                excel_path = get_embedded_excel_path(shape, slide)\n",
    "                if excel_path and excel_path in embedded_cache:\n",
    "                    sheet_dfs = embedded_cache[excel_path]\n",
    "                    wb_name = os.path.basename(excel_path)\n",
    "                    \n",
    "                    content.append(f\"\\n[Embedded workbook on Slide {slide_num}: {wb_name}]\")\n",
    "                    for sheet_name, df in sheet_dfs.items():\n",
    "                        content.append(f\"\\n--- EMBEDDED SHEET: {sheet_name} ---\")\n",
    "                        content.append(df.head(max_rows).to_csv(index=False).strip())\n",
    "                    slide_has_content = True\n",
    "\n",
    "        # Run the helper for all top-level shapes\n",
    "        for shape in slide.shapes:\n",
    "            extract_from_shape(shape)\n",
    "\n",
    "        # --------- Step 3: SmartArt / diagram text (plain text) ---------\n",
    "        raw_smartart_texts = extract_smartart_texts(slide)\n",
    "\n",
    "        if raw_smartart_texts:\n",
    "            content.append(f\"\\n[SMARTART content on this slide]\")\n",
    "            for t in raw_smartart_texts:\n",
    "                content.append(t)\n",
    "            slide_has_content = True\n",
    "\n",
    "        # --------- Step 4: Slide notes ---------\n",
    "        if slide.has_notes_slide:\n",
    "            try:\n",
    "                notes_text = slide.notes_slide.notes_text_frame.text.strip()\n",
    "                if notes_text:\n",
    "                    content.append(f\"\\n[Notes]: {notes_text}\")\n",
    "                    slide_has_content = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not slide_has_content:\n",
    "            content.append(\"[No text content found on this slide]\")\n",
    "\n",
    "    result = \"\\n\".join(content)\n",
    "\n",
    "    if not result.strip() or result.count(\"[No text content found on this slide]\") == len(prs.slides):\n",
    "        return f\"[PowerPoint file has {len(prs.slides)} slides but no extractable text was found]\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da650bf2-3604-4d19-98a8-e4b845f0a07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Processing all documents in the Azure storage folder\n",
    "\n",
    "This cell scans the configured Azure Data Lake directory and processes every supported file type, including Word, Excel, and PowerPoint documents.  \n",
    "For each file, the corresponding extraction function is applied to read text, tables, or embedded content.  \n",
    "All extracted information is combined into a single text output, which is saved back into the same storage location for later use in the pipeline.  \n",
    "The goal of this step is to unify the content of multiple document formats into one structured text file that can be passed to the AI model in the following stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef03f93b-4307-4cd7-8056-7e7d7f3bd7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files: ['TG3 Project Plan + Establishment v1.2.pptx']\n",
      "Wrote 23812 bytes.\n",
      "Combined content written to: abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/all_content.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define what file types we want to process\n",
    "extensions = [\".pdf\", \".pptx\", \".ppt\", \".docx\", \".doc\", \".xlsx\", \".xls\", \".csv\"]\n",
    "folder_path = INPUT_DIR\n",
    "\n",
    "# List all files in the folder\n",
    "items = dbutils.fs.ls(folder_path)\n",
    "\n",
    "# Filter to only get files with our extensions\n",
    "files = [item.path for item in items if any(item.name.lower().endswith(ext) for ext in extensions)]\n",
    "\n",
    "print(f\"Found {len(files)} files:\", [os.path.basename(f) for f in files])\n",
    "\n",
    "all_content = ''\n",
    "\n",
    "for i, file_path in enumerate(files):\n",
    "    all_content += f\"\\n\\nFile {i+1}: {os.path.basename(file_path)}\\n{'='*60}\\n\"\n",
    "                                                                                                \n",
    "    if file_path.lower().endswith('.docx'):\n",
    "        docx_content = read_docx_text(file_path)\n",
    "        all_content += docx_content + \"\\n\"\n",
    "    \n",
    "    elif file_path.lower().endswith(\".xlsx\"):\n",
    "        sheets = read_xlsx_sheets(file_path)\n",
    "        for sheet_name, df in sheets.items():\n",
    "            all_content += f\"\\n--- SHEET: {sheet_name} ---\\n\"\n",
    "            all_content += df.head().to_csv(index=False) + \"\\n\"\n",
    "\n",
    "    elif file_path.lower().endswith('.pptx'):\n",
    "        # Now this already includes embedded Excels in the correct slide position\n",
    "        pptx_content = read_pptx_text(file_path, include_embedded_excels=True, max_rows=30)\n",
    "        all_content += pptx_content + \"\\n\"\n",
    "\n",
    "    elif file_path.lower().endswith('.ppt'):\n",
    "        all_content += \"[.ppt format not supported by current extractor]\\n\"\n",
    "\n",
    "output_path = f\"{folder_path}/all_content.txt\"\n",
    "dbutils.fs.put(output_path, all_content, overwrite=True)\n",
    "\n",
    "print(f\"Combined content written to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561f4a5b-af34-48d1-94ba-3e0a735622be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File 1: TG3 Project Plan + Establishment v1.2.pptx\n",
      "============================================================\n",
      "\n",
      "--- Slide 1 ---\n",
      "TG3\u000bProject Plan + Establishment\u000b\u000bNordic Energy Data Quality Improvement – Phase 1\n",
      "Author: NordicTech Solutions AB – S&BS Team\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 2 ---\n",
      "TG3 Project Plan + Establishment\n",
      "Project Information:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Revision History: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appendices:\n",
      "Appendix 1: Meter Data Overview\n",
      "Appendix 2: Data Cleaning Rules\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Novision Criteria: \n",
      "<Mark Novision Criteria by changing to Checkmark Bullets>\n",
      "\n",
      "[Table in Slide 2]\n",
      "Project ID | PRJ-NE-DQP-001 / CAT4-E-0785\n",
      "Project Portfolio Owner | Emma Lindström\n",
      "Project Sponsor | Jonas Hallberg\n",
      "Project Manager | Maria Svensson\n",
      "+46 70 123 45 67\n",
      "maria.svensson@nordictechsolutions.com\n",
      "SAP code | Not applicable\n",
      "Project Period | 1 Nov 2025 – 28 Nov 2025\n",
      "\n",
      "[Table in Slide 2]\n",
      "Revision | Description of Change | Resp. | Effective Date\n",
      "1.0 | First version | T. Carbin | 2025-11-01 (project start / first approved plan)\n",
      "1.1 | Dummy data added to project plan sections | M. Svensson | 2025-11-08 (one week later, still early in the period)\n",
      "\n",
      "[Table in Slide 2]\n",
      " | Criteria | Comment\n",
      " | Strategic focus area compliance checked | Aligns with CO₂ reduction target\n",
      " | KPIs & Benefit realization detailed | KPIs created in scope section\n",
      " | Business Case detailed | Simple internal-cost model\n",
      " | Purpose, goals & deliverables detailed | Clear 3-task pilot scope\n",
      " | Scope/WBS & delimitations detailed | WBS provided in section 3\n",
      " | Requirements detailed | Minimal; access to meter files\n",
      " | Consequences, Dependencies, Risks & Handover detailed | Included in dedicated slide\n",
      " | Budget detailed and secured | €10,000 internal budget\n",
      " | Tender evaluation and final partner(s) selected | Not applicable\n",
      " | Solution implementation plan detailed | Timeline prepared\n",
      " | Partner contract & SLAs final commitment detailed | Not applicable\n",
      " | Organization allocated and approved | Small project team assigned\n",
      " | Communication Plan detailed | Weekly email updates\n",
      " | Support after Go-live detailed | Maintained by Energy Team\n",
      " | Lessons Learned documented | Will be added at TG4\n",
      "Current revision: 1.1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 3 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Project Description and Background: \n",
      "The project aims to improve the quality of electricity meter data for three pilot buildings within the Nordic region.\u000bCurrently, the data is stored in separate files with inconsistent formats, missing days, and manual corrections.\u000bThis pilot project focuses on cleaning and standardizing the data to prepare for future reporting and analytics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Purpose and Benefit Realization: \n",
      "The purpose is to establish reliable and consistent energy consumption data.\u000bThis improves reporting accuracy and supports the company’s CO₂ reduction and energy-efficiency goals.\n",
      "Benefits:\n",
      "Higher data accuracy\n",
      "Faster monthly reporting\n",
      "Foundation for future automation and dashboards\n",
      "KPIs:\n",
      "95% clean and validated data\n",
      "One combined dataset for all 3 buildings\n",
      "Monthly usage summary delivered\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Goals and Deliverables: \n",
      "\n",
      "Goals:\n",
      "Create a clean, unified dataset for 3 buildings\n",
      "Enable simple monthly reporting\n",
      "Ensure data reliability\n",
      "Deliverables:\n",
      "Cleaned dataset\n",
      "Data validation summary\n",
      "One monthly energy usage report\n",
      "\n",
      "\n",
      "\n",
      "Resource or Expertise needs:\n",
      "Project Manager – coordination and communication\n",
      "Data Analyst – data cleaning and validation\n",
      "IT Technician – access to meter data and source files\n",
      "Expertise required:\u000bBasic data analysis, Excel file handling, and understanding of meter data structure.\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 4 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Business Impact, Cost, Benefits and ROI: \n",
      "This pilot provides reliable energy data that supports sustainability reporting, cost efficiency, and CO₂ reduction strategy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cost:\u000bApprox. 75,000 SEK internal labor only.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Benefits:\n",
      "More reliable monthly energy reporting\n",
      "Reduced manual correction effort (approx. 10–12 hours saved per month)\n",
      "Faster availability of validated energy insights for operations\n",
      "Fewer reporting anomalies and less rework\n",
      "\n",
      "\n",
      "\n",
      "ROI:\u000bShort-term: Positive due to reduced manual work\n",
      "Long-term: High if expanded to additional buildings\n",
      "Break-even: Within the first reporting year\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Scope/WBS and Delimitations\n",
      "\n",
      "In Scope (3 simple tasks)\n",
      "Extract meter data from 3 buildings\n",
      "Clean & standardize the data\n",
      "Produce a monthly energy summary report\n",
      "\n",
      "Out of Scope\n",
      "Automation\n",
      "Real-time dashboards\n",
      "Predictive analytics\n",
      "Hardware upgrades\n",
      "\n",
      "WBS\n",
      "Data Extraction\n",
      "Data Cleaning\n",
      "Summary Reporting\n",
      "\n",
      "\n",
      "Requirements:\n",
      "Access to energy meter files for all 3 buildings\n",
      "Support from IT Technician for data retrieval\n",
      "Compliance with company data handling procedures\n",
      "Results should support CO₂ reduction and internal sustainability goals\n",
      "No special health or safety requirements\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 5 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Consequences:\n",
      "If the project is not executed:\n",
      "Monthly energy reporting will continue to rely on inconsistent and incomplete data.\n",
      "CO₂ reduction and energy-saving initiatives will lack accurate baseline data.\n",
      "Manual corrections will remain time-consuming and error-prone.\n",
      "Future automation and dashboard development will be delayed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dependencies:\n",
      "Access to meter data from Building A, B, and C.\n",
      "IT support to retrieve and share the raw data files.\n",
      "Energy Team availability to review the cleaned dataset.\n",
      "Shared folder structure for storing consolidated data.\n",
      "These dependencies are simple and minimal for this pilot phase.\n",
      "\n",
      "\n",
      "\n",
      "Risks: \n",
      "Missing or corrupted meter data for specific days.\n",
      "Delays in obtaining files from IT or building operations.\n",
      "Inconsistent data formats that require extra cleaning time.\n",
      "Limited availability of the Data Analyst during key steps.\n",
      "Risk exposure is low due to the small scope of the pilot.\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Strategic focus area(s): \n",
      "<mark applicable by changing to Checkmark Bullet(s)>\n",
      "1. We help our customer to reduce their CO2 footprint\n",
      "2. We develop our electricity business to meet new demands\n",
      "3. We build new business in flexibility services & integrated energy solutions\n",
      "4. We develop IT into a strategic advantage\n",
      "5. We engage our employees\n",
      "\n",
      "\n",
      "\n",
      "Project Governance and Handover: \n",
      "Project Portfolio Owner: Emma Lindström\n",
      "Handover Partner: Lars Nyberg (Energy Operations Lead)\n",
      "The cleaned dataset and monthly report will be handed over to the Energy Operations Team at project completion (expected 22–28 Nov 2025).\u000bThey will use this data for monthly reporting and sustainability analyses.\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 6 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Acceptance to include Project in Project Portfolio: \n",
      "Project Portfolio Owner PPO\n",
      "Emma Lindström\n",
      "<date>\t\t\n",
      "\n",
      "Project Sponsor\n",
      "Jonas Hallberg\n",
      "<date>\t\t\n",
      "\n",
      "Handover Partner\n",
      "Lars Nyberg\n",
      "<date>\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 7 ---\n",
      "Tender outcome:\n",
      "No external tendering process was required for this pilot phase.\u000bAll activities will be performed using internal resources from NordicTech Solutions AB, including data extraction, cleaning, and reporting.\u000bNo external vendors, software purchases, or contracts were involved.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Implementation plan:\n",
      "\n",
      "A simple 4-week implementation plan, aligned with the 3 tasks:\n",
      "Week 1 — Data Extraction (Completed)\n",
      "📅 1–7 Nov 2025\n",
      "IT Technician retrieves meter data from all 3 buildings\n",
      "Files validated and stored in shared workspace\n",
      "Week 2–3 — Data Cleaning (In Progress)\n",
      "📅 8–21 Nov 2025\n",
      "Combine and standardize the datasets\n",
      "Remove duplicates, fill gaps, validate consumption values\n",
      "Review with Energy Team\n",
      "Week 4 — Summary Report Preparation (Not Started)\n",
      "📅 22–28 Nov 2025\n",
      "Create monthly usage report\n",
      "Prepare clean dataset handover package\n",
      "Final review and closure meeting\n",
      "\n",
      "Internal stakeholders:\u000bProject Manager, Data Analyst, IT Technician, Energy Team\n",
      "\n",
      "External stakeholders:\u000bNone for this pilot project\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Contract, SLA and Payments\n",
      "As this is an internal pilot project, no external contract or SLA is required.\u000bAll work is executed under existing internal agreements and standard working procedures.\n",
      "\n",
      "Payment Terms:\n",
      "Total estimated cost: 75,000 SEK (internal labor only)\n",
      "No external supplier costs\n",
      "No invoices or payment schedules needed\n",
      "\n",
      "Scope and Quality:\n",
      "Deliverables: Clean dataset + monthly summary report\n",
      "Quality criteria: 95% validated and complete data\n",
      "Testing: Manual validation by Energy Team\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 8 ---\n",
      "Acceptance of Contract, SLA and Payment Terms: \n",
      "Project Portfolio Owner PPO\n",
      "Emma Lindström\n",
      "Approved on: 1 Nov 2025\n",
      "\n",
      "Project Sponsor\n",
      "Jonas Hallberg\n",
      "Approved on: 2 Nov 2025\n",
      "\n",
      "Handover Partner\n",
      "Lars Nyberg\n",
      "Approved on: 2 Nov 2025\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 9 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Risk Management:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "\n",
      "[Embedded workbook on Slide 9: Microsoft_Excel_Worksheet.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Nr,Description,Probability (1-5),Consequence (1-5),Value (1-25),Mitigation Action,Responsible,Due date\n",
      "1,IT Technician not available to retrieve meter data on time,3,3,9,Book technician time in advance and have a backup contact in IT,IT Technician,End of Week 1\n",
      "2,Delay in receiving meter data from one or more buildings,3,3,9,Send early data requests to building staff and follow up regularly,Project Manager,End of Week 1\n",
      "3,Missing or corrupted data in meter files,2,4,8,Use fallback sources where possible; document gaps and correct manually,Data Analyst,End of Week 3\n",
      "4,Inconsistent data formats between buildings,3,2,6,Define a standard format template and apply common cleaning rules,Data Analyst,End of Week 3\n",
      "5,Final report delayed due to review taking longer than planned,2,2,4,Schedule review slot with Energy Team in advance; share draft early,Handover,End of Week 4\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 10 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Project Organization:\n",
      "Organizational structure for the TG3 pilot project (Nov 2025)\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "[SMARTART content on this slide]\n",
      "Sponsor\n",
      "Jonas Hallberg\n",
      "Steering\n",
      "Group\n",
      "Elin Bergström\n",
      "Project Manager\n",
      "Maria Svensson\n",
      "IT Project Manager\n",
      "Daniel Larsson\n",
      "Project\n",
      "Member\n",
      "Sara Holm\n",
      "Project\n",
      "Member\n",
      "Mikael Jonsson\n",
      "Project Member (Data Analyst)\n",
      "Anna Karlsson\n",
      "Project Member (Energy Operations Rep)\n",
      "Lars Nyberg\n",
      "Project Member (IT Support Technician)\n",
      "Erik Johansson\n",
      "Sponsor\n",
      "Jonas Hallberg\n",
      "Project\n",
      "Member\n",
      "Sara Holm\n",
      "Project Manager\n",
      "Maria Svensson\n",
      "IT Project Manager\n",
      "Daniel Larsson\n",
      "Project Member (Data Analyst)\n",
      "Anna Karlsson\n",
      "Project\n",
      "Member\n",
      "Mikael Jonsson\n",
      "Project Member (Energy Operations Rep)\n",
      "Lars Nyberg\n",
      "Project Member (IT Support Technician)\n",
      "Erik Johansson\n",
      "Steering\n",
      "Group\n",
      "Elin Bergström\n",
      "\n",
      "--- Slide 11 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Work Breakdown Structure - WBS:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "Timeline:\n",
      "WP1.x tasks take place in Week 1 (1–7 Nov)\n",
      "WP2.x tasks take place in Weeks 2–3 (8–21 Nov)\n",
      "WP3.x tasks take place in Week 4 (22–28 Nov)\n",
      "\n",
      "[SMARTART content on this slide]\n",
      "Scope\n",
      "Area 1: Data Extraction\n",
      "Area 2: Data Cleaning & Standardization\n",
      "Area 3: Repoirting & Handover\n",
      "WP1.1 – Retrieve meter data from Building A\n",
      "WP1.2 – Retrieve meter data from Building B\n",
      "WP2.1 – Combine datasets into a unified format\n",
      "WP2.2 – Remove duplicates and fill misssing values\n",
      "WP3.1 – Create monthly energy summary report\n",
      "WP3.2 – Prepare clean dataset package\n",
      "WP1.3 – Retrieve meter data from Building C\n",
      "WP2.3 – Validate consumption values with Energy Team\n",
      "WP3.3 – Handover to Energy Operations Team\n",
      "Scope\n",
      "Area 1: Data Extraction\n",
      "WP1.1 – Retrieve meter data from Building A\n",
      "WP1.2 – Retrieve meter data from Building B\n",
      "WP1.3 – Retrieve meter data from Building C\n",
      "Area 2: Data Cleaning & Standardization\n",
      "WP2.1 – Combine datasets into a unified format\n",
      "WP2.2 – Remove duplicates and fill misssing values\n",
      "WP2.3 – Validate consumption values with Energy Team\n",
      "Area 3: Repoirting & Handover\n",
      "WP3.1 – Create monthly energy summary report\n",
      "WP3.2 – Prepare clean dataset package\n",
      "WP3.3 – Handover to Energy Operations Team\n",
      "\n",
      "--- Slide 12 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Network Diagram\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "Timeline:\n",
      "WP1.x tasks take place in Week 1 (1–7 Nov)\n",
      "WP2.x tasks take place in Weeks 2–3 (8–21 Nov)\n",
      "WP3.x tasks take place in Week 4 (22–28 Nov)\n",
      "\n",
      "[SMARTART content on this slide]\n",
      "Start\n",
      "WP1.1 – Retrieve meter data from Building A\n",
      "WP1.2 – Retrieve meter data from Building B\n",
      "WP1.3 – Retrieve meter data from C\n",
      "WP2.1 – Combine datasets into unified format\n",
      "WP2.2 – Clean missing/duplicate values\n",
      "WP2.3 – Validate cleaned data with Energy Team\n",
      "WP3.1 - Create monthly energy summary report\n",
      "WP3.2 - Prepare clean dataset package\n",
      "WP3.3 – Handover to Energy Operations\n",
      "Stop\n",
      "Start\n",
      "WP1.1 – Retrieve meter data from Building A\n",
      "WP1.2 – Retrieve meter data from Building B\n",
      "WP2.1 – Combine datasets into unified format\n",
      "WP2.2 – Clean missing/duplicate values\n",
      "WP2.3 – Validate cleaned data with Energy Team\n",
      "WP3.1 - Create monthly energy summary report\n",
      "WP3.3 – Handover to Energy Operations\n",
      "Stop\n",
      "WP3.2 - Prepare clean dataset package\n",
      "WP1.3 – Retrieve meter data from C\n",
      "\n",
      "--- Slide 13 ---\n",
      "\n",
      "[Embedded workbook on Slide 13: Microsoft_Excel_Worksheet1.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Nr,Activity,Description,Start (Date),Stop (Date),Duration (Hours/Days),Dependency\n",
      "1,Data Extraction,\"Retrieve meter data from Buildings A, B, C and check that files are complete\",2025-01-11 00:00:00,2025-07-11 00:00:00,7,Start\n",
      "2,Data Cleaning & Standardization,\"Combine datasets, remove missing/duplicate values, standardize columns\",2025-08-11 00:00:00,21/11/2025,14,Activity 1\n",
      "3,Reporting & Handover,Validate cleaned dataset with Energy Team,15/11/2025,21/11/2025,7,Activity 2 (Overlaps allowed)\n",
      "3a,Reporting – Summary Report,Create monthly summary report,22/11/2025,24/11/2025,3,Activity 2 & 3\n",
      "3b,Reporting – Dataset Package,Prepare clean dataset package,22/11/2025,26/11/2025,5,Activity 2 & 3\n",
      "3c,Final Handover,Final handover to Energy Operations,27/11/2025,28/11/2025,2,Activity 4 & 5\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Activity List:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 14 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Gantt Chart:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "\n",
      "[Table in Slide 14]\n",
      "Activity | Week 1 (Nov 1–7) | Week 2 (Nov 8–14) | Week 3 (Nov 15–21) | Week 4 (Jun 22–28)\n",
      "Data Extraction | (7 Days) |  |  | \n",
      "Data Cleaning & Standardization |  | (7 Days) | (7 Days) | \n",
      "Reporting & Handover |  |  |  | (7 Days)\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 15 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "SWOT Analysis:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "STRENGTHS (Internal, Helpful)\n",
      "Clear and limited projects scope (only 3 tasks)\n",
      "Uses internal resource only – no external dependencies\n",
      "Fast implementation (4 weeks total)\n",
      "Strong support from Energy Operations Team\n",
      "Improves data reliability immediately\n",
      "WEAKNESSES (Internal, Harmful)\n",
      "Manual data cleaning required\n",
      "Small project team may have competing priorities\n",
      "Depends on timely input from IT Technician and Energy Team\n",
      "Limited to 3 pilot buildings (not full company portfolio)\n",
      "OPPORTUNITIES (External, Helpful)\n",
      "Can extend to real-time monitoring dashboards in future phases\n",
      "Supports CO2 reduction and sustainability reporting needs\n",
      "Enables data-driven energy savings across more buildings\n",
      "Potential to automate cleaning and reporting in future phases\n",
      "Creates foundation for smarter energy analytics\n",
      "THREATS (External, Harmful)\n",
      "Delays from building operations or IT access issues\n",
      "Future meter hardware upgrades may change data formats\n",
      "Unexpected data quality issues may increase workload\n",
      "Other corporate priorities may limit resource availability\n",
      "HELPFUL\n",
      "HARMFUL\n",
      "EXTERNAL\n",
      "INTERNAL\n",
      "Status Report Date: 28 Nov 2025\n",
      "SWOT Analysis:\n",
      "\n",
      "--- Slide 16 ---\n",
      "\n",
      "[Embedded workbook on Slide 16: Microsoft_Excel_Worksheet2.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Stakeholder,Description of interest / impact / or need of information,Commitment\n",
      "Jonas Hallberg (Sponsor),Ensures project funding and approves final deliverables. Needs regular status updates.,1\n",
      "Maria Svensson (Project Manager),\"Coordinates tasks, ensures progress, manages risks, needs full visibility.\",1\n",
      "Daniel Larsson (IT Project Manager),Ensures technical access and supports IT coordination.,2\n",
      "Anna Karlsson (Data Analyst),\"Performs data cleaning and standardization, critical for task completion.\",2\n",
      "Lars Nyberg (Energy Operations Lead),\"Reviews cleaned dataset, receives final deliverables.\",3\n",
      "Erik Johansson (IT Technician),Retrieves meter files and ensures technical access.,3\n",
      "Energy Team (Operations),Uses the final dataset for reporting; needs final update only.,4\n",
      "\"Building Staff (A, B, C)\",Provides meter access if needed. Only requires minimal notifications.,5\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Stakeholder mapping:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 17 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Communication Plan:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "\n",
      "[Embedded workbook on Slide 17: Microsoft_Excel_Worksheet3.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Nr,How? (Meeting),When? (Frequency),What? (Agenda),Who? (Participants),Where? (Location)\n",
      "1,Project Team Meeting,\"Weekly, 30 min\",\"Progress updates, upcoming tasks, risks, data issues\",\"Project Manager, Data Analyst, IT Project Manager, IT Technician\",Teams\n",
      "2,Steering Group Update,\"Bi-weekly, 20 min\",\"Status, risks, decisions needed\",\"Project Manager, Sponsor, Steering Group\",Teams\n",
      "3,Data Review Session,At completion of data cleaning,\"Review cleaned dataset, validate results, confirm correctness\",\"Project Manager, Data Analyst, Energy Operations Lead\",On site or Teams\n",
      "4,Final Handover Meeting,End of Week 4,\"Present summary report, deliver clean dataset, confirm transition\",\"Project Manager, Energy Operations Team, Sponsor\",On site or Teams\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 18 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Handover Management:\n",
      "The Project Manager hands over the cleaned dataset and summary report to the Handover Manager in the Operating Unit once TG4 is approved.\u000bThe Handover Manager is responsible for ensuring that the Energy Operations team receives all updated documentation and that the data is ready for ongoing monthly reporting.\n",
      "The Operating Unit Manager must ensure resources are available for the transition and appoint a Super User who will support daily operations after the project ends. The Super User will act as the local contact for any questions regarding the cleaned dataset and reporting process.\n",
      "The Sponsor approves the final delivery and ensures that expected benefits (improved data quality, smoother reporting, reduced manual corrections) are realized over time.\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Project Organization\n",
      "Operating Unit\n",
      "Maintenance\n",
      "Handover Manager\n",
      "Super User\n",
      "Manager\n",
      "Henrik Olofsson\n",
      "Sofia Bergström\n",
      "Johan Wiklund\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "[SMARTART content on this slide]\n",
      "Sponsor\n",
      "[Jonas Hallberg]\n",
      "Project Manager\n",
      "Maria Svensson\n",
      "IT Project Manager\n",
      "Daniel Larsson]\n",
      "Project Member (Energy Ops)\n",
      "Lars Nyberg\n",
      "Project Member\n",
      "(Data Analyst)\n",
      "Anna Karlsson\n",
      "Steering Group\n",
      "Eva Lundström\n",
      "Project Member (IT Technician)\n",
      "Erik Johansson\n",
      "Sponsor\n",
      "[Jonas Hallberg]\n",
      "Steering Group\n",
      "Eva Lundström\n",
      "Project Manager\n",
      "Maria Svensson\n",
      "IT Project Manager\n",
      "Daniel Larsson]\n",
      "Project Member\n",
      "(Data Analyst)\n",
      "Anna Karlsson\n",
      "Project Member (Energy Ops)\n",
      "Lars Nyberg\n",
      "Project Member (IT Technician)\n",
      "Erik Johansson\n",
      "\n",
      "--- Slide 19 ---\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Lessons Learned:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "\n",
      "[Embedded workbook on Slide 19: Microsoft_Excel_Worksheet4.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Nr,Area,What went well?,What could have been done better?\n",
      "1,Project Planning,\"Scope was clear, timeline was easy to follow, and responsibilities were well-defined.\",Earlier alignment with the Operating Unit could have reduced last-minute questions.\n",
      "2,Data Extraction,\"Meter data from Buildings A, B, and C was retrieved smoothly with good support from IT.\",Some file structures were inconsistent; a standard format request could have saved time.\n",
      "3,Data Cleaning,Cleaning and standardization tools worked efficiently; few technical issues occurred.,More automation could reduce manual steps in future phases.\n",
      "4,Collaboration,\"Communication between PM, IT, and Energy Ops was effective and timely.\",Availability of IT Technician was limited on some days; early scheduling could help.\n",
      "5,Reporting & Handover,Final dataset and summary report were delivered on time and accepted without major changes.,More visual dashboards could be added to make insights clearer for stakeholders.\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 20 ---\n",
      "\n",
      "[Embedded workbook on Slide 20: Microsoft_Excel_Worksheet5.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Nr,Activity,Description,Deadline,Responsible,Status\n",
      "1,Dashboard Prototype,Create a simple Power BI dashboard using the cleaned dataset (Phase 2 item),15/01/2026,Data Analyst (Anna Karlsson),Not started\n",
      "2,Automation Proposal,Prepare a short proposal for automating monthly data cleaning (future improvement),2026-12-01 00:00:00,Project Manager (Maria Svensson),In progress\n",
      "3,Data Dictionary,\"Document each column, units, and data rules for long-term usage\",2026-10-01 00:00:00,Energy Ops Lead (Lars Nyberg),Not started\n",
      "TG3 Project Plan + Establishment\u000bProject Period: 1 Nov 2025 – 28 Nov 2025\n",
      "Rest Activities:\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n",
      "--- Slide 21 ---\n",
      "\n",
      "[Embedded workbook on Slide 21: Microsoft_Excel_Worksheet6.xlsx]\n",
      "\n",
      "--- EMBEDDED SHEET: Sheet1 ---\n",
      "Nr,Cost Category,Planned (SEK),Actual (SEK),Difference,Status\n",
      "1,Personnel Cost,60000,100000,-40000,Not started\n",
      "2,IT Support / Access,10000,11000,-1000,In progress\n",
      "3,Reporting & Documentation,5000,4000,1000,Not started\n",
      "4,Total,75000,115000,-40000,\n",
      "TG3 Project Plan + Establishment\n",
      "Financial Performance\n",
      "Nordic Energy Data Quality Improvement – Phase 1\n",
      "Benefits / Value (Financial + Operational)\n",
      "Value Delivered\n",
      "Reduced manual data cleaning time by ~12 hours/month\n",
      "Improved data quality → fewer reporting errors\n",
      "Potential long-term savings through automation\n",
      "Faster monthly reporting process\n",
      ".\n",
      "ROI Snapshot\n",
      "ROI Assessment\n",
      "Short-term ROI: Moderate, due to small pilot scope\n",
      "Long-term ROI: High, if extended to more buildings and \n",
      "Break-even: Achievable within the first full reporting cycle if scaled\n",
      "Status Report Date: 28 Nov 2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True: # set to True to test\n",
    "    output_path = f\"{INPUT_DIR}/all_content.txt\"\n",
    "    data = dbutils.fs.head(output_path, 100000000)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46697db6-bfa6-48ad-836c-c65f7b6a407b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a project management data analyst extracting statuses from project documents. I will provide text extracted from a project management document, and you will extract status information from it.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Only extract information explicitly present in the provided document.\n",
    "- Do not make assumptions, estimates, or guesses.\n",
    "- Do not invent or hallucinate values.\n",
    "- If information is missing or unclear, do not generate a KPI for it.\n",
    "- Keep wording and numbers as close as possible to the original source.\n",
    "\n",
    "Pay attention to Status Report Date. Analyze the project and return:\n",
    "\n",
    "1. A financial status score between 0 and 1, and a concise explanation of why\n",
    "2. A timing status score between 0 and 1, and a concise explanation that explicitly uses the Status Report Date, compares the planned schedule (as described in the implementation plan in the document) with the actual progress reported, and clearly states whether the project is on track, slightly behind, significantly behind, or ahead of schedule, including a short justification for the chosen score.\n",
    "3. The resource status must evaluate whether the project has the required roles, people, and support available as described in the document. The explanation should compare planned resource availability versus actual availability or constraints mentioned in the text, and state whether resources are sufficient, partially insufficient, or blocking the progress and mention based on which risks the value estimated. The score (0–1) should reflect this assessment.\n",
    "4. A task status score between 0 and 1, and a concise explanation of why\n",
    "5. An overall status score between 0 and 1, and a concise explanation of why\n",
    "\n",
    "Respond only in valid JSON using this structure (no extra text, comments, or formatting outside the JSON):\n",
    "\n",
    "{{\n",
    "  \"Financial\": {{\n",
    "    \"StatusDescription\": \"<short explanation of financial status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "  \"Timing\": {{\n",
    "    \"StatusDescription\": \"<short explanation of timing status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "  \"Resources\": {{\n",
    "    \"StatusDescription\": \"<short explanation of resource status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "\n",
    "    \"Tasks\": {{\n",
    "    \"Tasks Description In Project\": \"<short explanation of overall status with bulletpoints>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "\n",
    "  \"Overall\": {{\n",
    "    \"StatusDescription\": \"<short explanation of overall status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Here is the project management document’s extracted text:\n",
    "{all_content}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daeea6c-13b7-4803-815e-5aaddf6aa8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DEPLOYMENT = \"gpt-4o-sweden\"\n",
    "# DEPLOYMENT = \"gpt-5-chat-latest\"\n",
    "response = client.chat.completions.create(\n",
    "    model=DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a project management analyst.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed17e0f-8561-4d2e-a50c-57614b1e8da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4\n",
      "gpt-3.5-turbo\n",
      "gpt-5.1-codex-max\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "davinci-002\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4-1106-preview\n",
      "gpt-3.5-turbo-1106\n",
      "tts-1-hd\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-realtime-preview\n",
      "omni-moderation-latest\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o1-2024-12-17\n",
      "o1\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "o1-pro-2025-03-19\n",
      "o1-pro\n",
      "gpt-4o-mini-tts\n",
      "o3-2025-04-16\n",
      "o4-mini-2025-04-16\n",
      "o3\n",
      "o4-mini\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-image-1\n",
      "codex-mini-latest\n",
      "o3-pro\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "o3-pro-2025-06-10\n",
      "o4-mini-deep-research\n",
      "o3-deep-research\n",
      "gpt-4o-transcribe-diarize\n",
      "o3-deep-research-2025-06-26\n",
      "o4-mini-deep-research-2025-06-26\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-audio-2025-08-28\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-audio\n",
      "gpt-5-codex\n",
      "gpt-image-1-mini\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-pro\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-5-search-api\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "sora-2\n",
      "sora-2-pro\n",
      "gpt-5-search-api-2025-10-14\n",
      "gpt-5.1-chat-latest\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1\n",
      "whisper-1\n",
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fefcb5-ba48-4040-b2e0-3fcb9805365f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1612 bytes.\n",
      "Output JSON saved to: abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "raw_output = response.choices[0].message.content.strip()\n",
    "# if the model returens markdown code blocks, remove them\n",
    "if raw_output.startswith(\"```\"):\n",
    "    raw_output = re.sub(r'^```json?\\s*', '', raw_output)\n",
    "    raw_output = re.sub(r'```\\s*$', '', raw_output)    \n",
    "\n",
    "output = json.loads(raw_output)\n",
    "output_path = f\"{INPUT_DIR}/output.json\"\n",
    "output_json = json.dumps(output, indent=4, ensure_ascii=False)\n",
    "dbutils.fs.put(output_path, output_json, overwrite=True)\n",
    "print(f\"Output JSON saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588f00f5-ff95-4d66-a7f7-80ffe0a7fb07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Financial': {'StatusDescription': 'Actual cost 115,000\\u202fSEK vs planned 75,000\\u202fSEK (≈40,000\\u202fSEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.',\n",
       "  'Status': 0.4},\n",
       " 'Timing': {'StatusDescription': 'Status Report Date 28\\u202fNov\\u202f2025 matches planned project end (1–28\\u202fNov\\u202f2025). Implementation plan shows Week\\u202f1 Data Extraction completed, Weeks\\u202f2–3 Data Cleaning in progress, Week\\u202f4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.',\n",
       "  'Status': 0.7},\n",
       " 'Resources': {'StatusDescription': 'Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.',\n",
       "  'Status': 0.8},\n",
       " 'Tasks': {'Tasks Description In Project': '• Data Extraction – Completed\\n• Data Cleaning & Standardization – In Progress\\n• Summary Report Preparation – Not Started (as of 28\\u202fNov\\u202f2025)\\n• Handover – Planned end of Week\\u202f4',\n",
       "  'Status': 0.6},\n",
       " 'Overall': {'StatusDescription': 'Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.',\n",
       "  'Status': 0.6}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_str = dbutils.fs.head(f\"{INPUT_DIR}/output.json\", 100000)\n",
    "raw = json.loads(json_str)\n",
    "\n",
    "display(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4630b5-900f-4d3a-8f4f-9b91cac3088b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "rows = [\n",
    "    Row(\n",
    "        kpi_name=\"Financial\",\n",
    "        status_description=raw[\"Financial\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Financial\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Timing\",\n",
    "        status_description=raw[\"Timing\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Timing\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Resources\",\n",
    "        status_description=raw[\"Resources\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Resources\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Overall\",\n",
    "        status_description=raw[\"Overall\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Overall\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Tasks\",\n",
    "        status_description=raw[\"Tasks\"][\"Tasks Description In Project\"],\n",
    "        status=float(raw[\"Tasks\"][\"Status\"])\n",
    "    )\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "df = df.withColumn(\"ingested_at\", F.current_timestamp())\n",
    "\n",
    "# Write to a new Delta table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"project_status_kpi_long\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93295bf7-148d-439e-bae6-2b3cc9f782a3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763645814203}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>kpi_name</th><th>status_description</th><th>status</th><th>ingested_at</th></tr></thead><tbody><tr><td>Tasks</td><td>• Data Extraction – Completed\n",
       "• Data Cleaning & Standardization – In Progress\n",
       "• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n",
       "• Handover – Planned end of Week 4</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Resources</td><td>Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.</td><td>0.8</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Timing</td><td>Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.</td><td>0.7</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Overall</td><td>Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Financial</td><td>Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.</td><td>0.4</td><td>2025-12-05T15:02:30.63Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Tasks",
         "• Data Extraction – Completed\n• Data Cleaning & Standardization – In Progress\n• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n• Handover – Planned end of Week 4",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Resources",
         "Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.",
         0.8,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Timing",
         "Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.",
         0.7,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Overall",
         "Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Financial",
         "Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.",
         0.4,
         "2025-12-05T15:02:30.63Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 25
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "kpi_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ingested_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM project_status_kpi_long\n",
    "ORDER BY ingested_at DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425cce71-eba5-4869-a59e-f36970665da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>kpi_name</th><th>status_description</th><th>status</th><th>ingested_at</th></tr></thead><tbody><tr><td>Timing</td><td>Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.</td><td>0.7</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Resources</td><td>Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.</td><td>0.8</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Overall</td><td>Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Tasks</td><td>• Data Extraction – Completed\n",
       "• Data Cleaning & Standardization – In Progress\n",
       "• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n",
       "• Handover – Planned end of Week 4</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Financial</td><td>Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.</td><td>0.4</td><td>2025-12-05T15:02:30.63Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Timing",
         "Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.",
         0.7,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Resources",
         "Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.",
         0.8,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Overall",
         "Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Tasks",
         "• Data Extraction – Completed\n• Data Cleaning & Standardization – In Progress\n• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n• Handover – Planned end of Week 4",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Financial",
         "Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.",
         0.4,
         "2025-12-05T15:02:30.63Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 26
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "kpi_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ingested_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES;\n",
    "SELECT * FROM project_status_kpi_long;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef9bb4a2-bfa6-48ea-aaac-4a884fbd32da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8866569949184394,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_extraction_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
