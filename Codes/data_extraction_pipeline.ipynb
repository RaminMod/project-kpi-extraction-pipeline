{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa87ea98-482b-4dea-80d0-5ba86cfa4618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Installing required libraries\n",
    "\n",
    "In this cell I'm just preparing the environment for the whole extraction pipeline.  \n",
    "I upgrade `pip` and install the libraries I need to read PowerPoint, Word, and Excel files, work with tables using pandas, and call the OpenAI model.  \n",
    "Nothing complicated here — just making sure everything is ready for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc45795f-294a-4f37-b952-397c5ccbe3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (25.3)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: openai==1.55.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (1.55.3)\nRequirement already satisfied: httpx==0.27.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: python-pptx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (1.0.2)\nRequirement already satisfied: python-docx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (1.2.0)\nRequirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (3.1.5)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.10/site-packages (from openai==1.55.3) (3.5.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.55.3) (1.7.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openai==1.55.3) (0.12.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.10/site-packages (from openai==1.55.3) (1.10.6)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from openai==1.55.3) (1.2.0)\nRequirement already satisfied: tqdm>4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openai==1.55.3) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openai==1.55.3) (4.15.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx==0.27.2) (2022.12.7)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from httpx==0.27.2) (1.0.9)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from httpx==0.27.2) (3.4)\nRequirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\nRequirement already satisfied: Pillow>=3.3.2 in /databricks/python3/lib/python3.10/site-packages (from python-pptx) (9.4.0)\nRequirement already satisfied: XlsxWriter>=0.5.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from python-pptx) (3.2.9)\nRequirement already satisfied: lxml>=3.1.0 in /databricks/python3/lib/python3.10/site-packages (from python-pptx) (4.9.1)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d98a6bd4-2a87-4c5a-ad9b-e2d3eca0ce19/lib/python3.10/site-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2022.7)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install openai==1.55.3 httpx==0.27.2 python-pptx python-docx openpyxl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1439687b-48c3-4928-a65c-597e9a4702ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70958f55-7caf-4a6c-a6fb-3a29dfe909c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) Initializing the Azure OpenAI client\n",
    "\n",
    "This cell sets up the Azure OpenAI configuration by defining the endpoint, API key, and client object used for sending requests to the GPT model.  \n",
    "These settings allow the notebook to communicate with the Azure OpenAI deployment throughout the extraction and analysis workflow.  \n",
    "For production use, the API key should be stored securely in a Databricks secret scope rather than directly in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7ecd75-229c-46c4-ac77-33efcaa3223c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Read secrets from Databricks Secret Scope\n",
    "AZURE_OPENAI_ENDPOINT = dbutils.secrets.get(\"itera-secrets\", \"AZURE-OPENAI-ENDPOINT\")\n",
    "AZURE_OPENAI_KEY = dbutils.secrets.get(\"itera-secrets\", \"AZURE-OPENAI-KEY\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91321e69-065d-467e-a693-7206968ab0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuring SAS access for Azure Data Lake (ABFS)\n",
    "\n",
    "This cell sets up Spark to authenticate against Azure Data Lake Gen2 using a SAS token.  \n",
    "The configuration enables the notebook to read and process files stored in the specified container by registering the SAS token with the ABFS driver.  \n",
    "A base `abfss://` path is also built here, which is reused in the following cells when loading documents from storage.  \n",
    "For long-term or production use, the SAS token should be stored securely (for example, in a Databricks secret scope) instead of being written directly in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3968b7d3-6a1e-421f-91e9-1c7f7e6b4578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read SAS token securely from Databricks secret scope\n",
    "SAS_TOKEN = dbutils.secrets.get(\"itera-secrets\", \"ADLS-SAS-TOKEN\")\n",
    "ACCOUNT   = \"iterastorerm\"\n",
    "CONTAINER = \"itera-extraction-project\"\n",
    "\n",
    "# Configure Spark to use the SAS for ABFS (Azure Data Lake Gen2)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{ACCOUNT}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.token.provider.type.{ACCOUNT}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "# lstrip('?') removes the leading '?' so Spark gets only the token body\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.fixed.token.{ACCOUNT}.dfs.core.windows.net\",\n",
    "    SAS_TOKEN.lstrip(\"?\")\n",
    ")\n",
    "\n",
    "# Build the base ABFSS path used in the next cells\n",
    "BASE_ABFSS = f\"abfss://{CONTAINER}@{ACCOUNT}.dfs.core.windows.net\"\n",
    "INPUT_DIR  = BASE_ABFSS\n",
    "print(\"Configured. Base path:\", BASE_ABFSS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b4004e-43b6-45e5-a4ac-727b8b644fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/TG3 Project Plan + Establishment v1.2.pptx</td><td>TG3 Project Plan + Establishment v1.2.pptx</td><td>535197</td><td>1764943162000</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/all_content.txt</td><td>all_content.txt</td><td>23812</td><td>1764945519000</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/output.json</td><td>output.json</td><td>1415</td><td>1764945524000</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/practice/</td><td>practice/</td><td>0</td><td>0</td></tr><tr><td>abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/resources_kpi.json</td><td>resources_kpi.json</td><td>20</td><td>1763563201000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/TG3 Project Plan + Establishment v1.2.pptx",
         "TG3 Project Plan + Establishment v1.2.pptx",
         535197,
         1764943162000
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/all_content.txt",
         "all_content.txt",
         23812,
         1764945519000
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/output.json",
         "output.json",
         1415,
         1764945524000
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/practice/",
         "practice/",
         0,
         0
        ],
        [
         "abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/resources_kpi.json",
         "resources_kpi.json",
         20,
         1763563201000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing files in the storage container\n",
    "display(dbutils.fs.ls(INPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a1eff1-8c5f-4453-aab2-7a3210095289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Defining helper functions for reading DOCX, XLSX, and PPTX files\n",
    "\n",
    "In the next few cells, helper functions are defined for extracting content from Word, Excel, and PowerPoint files stored in Azure Data Lake.\n",
    "\n",
    "These functions together make it possible to:\n",
    "- Load binary files directly from ABFS using Spark.\n",
    "- Read text and tables from Word (.docx) files.\n",
    "- Read data from all sheets inside Excel (.xlsx) files.\n",
    "- Parse PowerPoint (.pptx) slides and extract slide text, native tables, slide notes, and detect embedded Excel objects.\n",
    "- When embedded Excel workbooks exist inside a slide, extract them and place the content directly under the slide where they appear, so the structure of the presentation is kept.\n",
    "\n",
    "Even though the functions are split across separate cells, they work together as one toolbox and form the core of the data-extraction pipeline, allowing the notebook to turn different document formats into structured, readable output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19b60ab-f2c6-4255-9953-52dfabd37b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing all required libraries for reading, parsing, and processing DOCX, XLSX, and PPTX files.\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "from docx import Document\n",
    "from openpyxl import load_workbook\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3325d99-2299-4dce-8d20-821b5a82ab3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AZURE DATA LAKE FILE READER\n",
    "def _read_bytes_abfs(path):\n",
    "    \"\"\"\n",
    "    Read a file from Azure Blob File System (ABFS) and return its raw bytes.\n",
    "    \n",
    "    Args:\n",
    "        path (str): ABFS path to the file\n",
    "        \n",
    "    Returns:\n",
    "        bytes: Raw file content as bytes\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist at the given path\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"binaryFile\").load(path)\n",
    "    rows = df.collect()\n",
    "    \n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    \n",
    "    return rows[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb186618-652a-4ef5-9695-15b8627f1c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WORD DOCUMENT (.docx) EXTRACTION\n",
    "def read_docx_text(abfs_path, include_tables=False):\n",
    "    \"\"\"\n",
    "    Extract all text content from a Word document, optionally including tables.\n",
    "    \n",
    "    Args:\n",
    "        abfs_path (str): Path to the .docx file in Azure Data Lake\n",
    "        include_tables (bool): If True, extracts table content as well\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text with paragraphs and tables (if requested)\n",
    "        \n",
    "    Example:\n",
    "        >>> text = read_docx_text(\"abfs://container/document.docx\", include_tables=True)\n",
    "        >>> print(text)\n",
    "    \"\"\"\n",
    "    # Load the Word document from Azure\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "    doc = Document(io.BytesIO(data))\n",
    "    \n",
    "    content = []\n",
    "    \n",
    "    # Extract all paragraphs\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        if text:  # Only include non-empty paragraphs\n",
    "            content.append(text)\n",
    "    \n",
    "    # Extract tables if requested\n",
    "    if include_tables:\n",
    "        for i, table in enumerate(doc.tables):\n",
    "            content.append(f\"\\n--- Table {i + 1} ---\")\n",
    "            \n",
    "            for row in table.rows:\n",
    "                # Get text from each cell in the row\n",
    "                cells = [cell.text.strip() for cell in row.cells]\n",
    "                \n",
    "                # Only include rows that have some content\n",
    "                if any(cells):\n",
    "                    content.append(\" | \".join(cells))\n",
    "    \n",
    "    return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e966ad-3aa1-4039-a14e-784065acae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EXCEL WORKBOOK (.xlsx) EXTRACTION\n",
    "def read_xlsx_sheets(abfs_path):\n",
    "    \"\"\"\n",
    "    Read all sheets from an Excel workbook into pandas DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        abfs_path (str): Path to the .xlsx file in Azure Data Lake\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with sheet names as keys and DataFrames as values\n",
    "              Example: {'Sheet1': DataFrame, 'Sales Data': DataFrame}\n",
    "              \n",
    "    Example:\n",
    "        >>> sheets = read_xlsx_sheets(\"abfs://container/data.xlsx\")\n",
    "        >>> for sheet_name, df in sheets.items():\n",
    "        ...     print(f\"{sheet_name}: {len(df)} rows\")\n",
    "    \"\"\"\n",
    "    # Load the Excel file from Azure\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "    workbook = load_workbook(io.BytesIO(data), data_only=True)\n",
    "\n",
    "    sheets = {}\n",
    "    \n",
    "    for sheet in workbook.worksheets:\n",
    "        # Get all cell values from the sheet\n",
    "        rows = list(sheet.values)\n",
    "        \n",
    "        if not rows:\n",
    "            continue  # Skip empty sheets\n",
    "        \n",
    "        # First row is assumed to be the header\n",
    "        header = rows[0]\n",
    "        data_rows = rows[1:]\n",
    "        \n",
    "        # Handle cases where headers are missing or all None\n",
    "        if not header or all(h is None for h in header):\n",
    "            num_cols = len(data_rows[0]) if data_rows else 0\n",
    "            header = [f\"Column_{i+1}\" for i in range(num_cols)]\n",
    "        \n",
    "        # Create DataFrame from the data\n",
    "        df = pd.DataFrame(data_rows, columns=header)\n",
    "        \n",
    "        # Remove completely empty rows (all NaN values)\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        sheets[sheet.title] = df\n",
    "    \n",
    "    return sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fa5d93-2d0d-4114-b0dd-24942aa23a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# POWERPOINT EMBEDDED EXCEL EXTRACTION (STANDALONE)\n",
    "def read_pptx_embedded_excels(abfs_path):\n",
    "    \"\"\"\n",
    "    Extract all embedded Excel workbooks from a PowerPoint file.\n",
    "    \n",
    "    This function is useful when you only want the Excel data without\n",
    "    the PowerPoint text content.\n",
    "    \n",
    "    Args:\n",
    "        abfs_path (str): Path to the .pptx file in Azure Data Lake\n",
    "        \n",
    "    Returns:\n",
    "        dict: Nested dictionary structure:\n",
    "              {\n",
    "                  'ppt/embeddings/embeddedWorkbook1.xlsx': {\n",
    "                      'Sheet1': DataFrame,\n",
    "                      'Sheet2': DataFrame\n",
    "                  },\n",
    "                  'ppt/embeddings/embeddedWorkbook2.xlsx': {\n",
    "                      'Data': DataFrame\n",
    "                  }\n",
    "              }\n",
    "              \n",
    "    Example:\n",
    "        >>> embedded = read_pptx_embedded_excels(\"abfs://container/presentation.pptx\")\n",
    "        >>> for file_path, sheets in embedded.items():\n",
    "        ...     print(f\"Found: {file_path}\")\n",
    "    \"\"\"\n",
    "    # Load the PowerPoint file from Azure\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "    embedded = {}\n",
    "\n",
    "    # PowerPoint files are actually ZIP archives - open as such\n",
    "    with zipfile.ZipFile(io.BytesIO(data)) as z:\n",
    "        # Find all embedded Excel files in the ZIP structure\n",
    "        excel_files = [\n",
    "            name for name in z.namelist()\n",
    "            if name.startswith(\"ppt/embeddings/\")\n",
    "            and (name.endswith(\".xlsx\") or name.endswith(\".xlsm\"))\n",
    "        ]\n",
    "\n",
    "        # Process each embedded Excel file\n",
    "        for name in excel_files:\n",
    "            wb_bytes = z.read(name)\n",
    "            wb = load_workbook(io.BytesIO(wb_bytes), data_only=True)\n",
    "\n",
    "            sheet_dfs = {}\n",
    "            \n",
    "            for sheet in wb.worksheets:\n",
    "                rows = list(sheet.values)\n",
    "                \n",
    "                if not rows:\n",
    "                    continue  # Skip empty sheets\n",
    "                \n",
    "                header = rows[0]\n",
    "                data_rows = rows[1:]\n",
    "\n",
    "                # Generate generic headers if missing\n",
    "                if not header or all(h is None for h in header):\n",
    "                    num_cols = len(data_rows[0]) if data_rows else 0\n",
    "                    header = [f\"Column_{i+1}\" for i in range(num_cols)]\n",
    "\n",
    "                df = pd.DataFrame(data_rows, columns=header)\n",
    "                df = df.dropna(how=\"all\")\n",
    "                sheet_dfs[sheet.title] = df\n",
    "\n",
    "            # Only include workbooks that have at least one non-empty sheet\n",
    "            if sheet_dfs:\n",
    "                embedded[name] = sheet_dfs\n",
    "\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d9e7fb-1b70-4911-b759-0903b807c07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_smartart_texts(slide):\n",
    "    \"\"\"\n",
    "    Extract raw SmartArt texts (Sponsor, Project Manager, names, etc.)\n",
    "    from the SmartArt XML parts of a slide.\n",
    "    Returns a flat list of strings in the same order as in the SmartArt pane.\n",
    "    \"\"\"\n",
    "    smartart_texts = []\n",
    "    slide_part = slide.part\n",
    "\n",
    "    for rel in slide_part.rels.values():\n",
    "        if \"diagram\" in rel.reltype:  # SmartArt / diagram parts\n",
    "            try:\n",
    "                part = rel.target_part\n",
    "                root = ET.fromstring(part.blob)\n",
    "                for el in root.iter():\n",
    "                    # SmartArt text nodes are typically <dgm:t>\n",
    "                    if el.tag.endswith(\"}t\") and el.text and el.text.strip():\n",
    "                        smartart_texts.append(el.text.strip())\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return smartart_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b419f0-d331-4c80-9b11-b5c7d4db1bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_pptx_text(abfs_path, include_embedded_excels=True, max_rows=10):\n",
    "    \"\"\"\n",
    "    Read text from a PowerPoint presentation.\n",
    "    - Extracts slide text (including grouped shapes), native tables, notes.\n",
    "    - Extracts embedded Excel workbooks and attaches them under the slide.\n",
    "    - Extracts SmartArt / diagram text per slide and outputs JSON-structured blocks.\n",
    "    \"\"\"\n",
    "    data = _read_bytes_abfs(abfs_path)\n",
    "\n",
    "    # --------- Step 1: Load all embedded Excel workbooks into a cache ---------\n",
    "    embedded_cache = {}\n",
    "    if include_embedded_excels:\n",
    "        with zipfile.ZipFile(io.BytesIO(data)) as z:\n",
    "            excel_files = [\n",
    "                name for name in z.namelist()\n",
    "                if name.startswith(\"ppt/embeddings/\")\n",
    "                and (name.endswith(\".xlsx\") or name.endswith(\".xlsm\"))\n",
    "            ]\n",
    "            \n",
    "            for name in excel_files:\n",
    "                wb_bytes = z.read(name)\n",
    "                wb = load_workbook(io.BytesIO(wb_bytes), data_only=True)\n",
    "\n",
    "                sheet_dfs = {}\n",
    "                for sheet in wb.worksheets:\n",
    "                    rows = list(sheet.values)\n",
    "                    if not rows:\n",
    "                        continue\n",
    "\n",
    "                    header = rows[0]\n",
    "                    data_rows = rows[1:]\n",
    "\n",
    "                    if not header or all(h is None for h in header):\n",
    "                        num_cols = len(data_rows[0]) if data_rows else 0\n",
    "                        header = [f\"Column_{i+1}\" for i in range(num_cols)]\n",
    "\n",
    "                    df = pd.DataFrame(data_rows, columns=header)\n",
    "                    df = df.dropna(how=\"all\")\n",
    "                    sheet_dfs[sheet.title] = df\n",
    "\n",
    "                if sheet_dfs:\n",
    "                    # Store by filename for lookup\n",
    "                    embedded_cache[name] = sheet_dfs\n",
    "\n",
    "    # --------- Step 2: Parse slides and match embedded files via relationships ---------\n",
    "    prs = Presentation(io.BytesIO(data))\n",
    "    content = []\n",
    "\n",
    "    for slide_num, slide in enumerate(prs.slides, start=1):\n",
    "        content.append(f\"\\n--- Slide {slide_num} ---\")\n",
    "        slide_has_content = False\n",
    "\n",
    "        # Helper to extract embedded file path from shape relationships\n",
    "        def get_embedded_excel_path(shape, slide):\n",
    "            \"\"\"Get the path to embedded Excel file for this specific shape\"\"\"\n",
    "            try:\n",
    "                # Check if shape has an OLE object\n",
    "                if \"oleObj\" not in shape._element.xml:\n",
    "                    return None\n",
    "                \n",
    "                # Parse the shape XML to find the relationship ID\n",
    "                shape_xml = shape._element.xml\n",
    "                \n",
    "                # Look for r:id or r:embed attributes\n",
    "                rid_match = re.search(r'r:id=\"(rId\\d+)\"', shape_xml)\n",
    "                if not rid_match:\n",
    "                    rid_match = re.search(r'r:embed=\"(rId\\d+)\"', shape_xml)\n",
    "                \n",
    "                if not rid_match:\n",
    "                    return None\n",
    "                \n",
    "                rid = rid_match.group(1)\n",
    "                \n",
    "                # Get the actual file path from slide relationships\n",
    "                slide_part = slide.part\n",
    "                if rid in slide_part.rels:\n",
    "                    rel = slide_part.rels[rid]\n",
    "                    target = rel.target_ref\n",
    "                    \n",
    "                    # Convert relative path to full path\n",
    "                    if target.startswith(\"../embeddings/\"):\n",
    "                        return f\"ppt/embeddings/{target.split('/')[-1]}\"\n",
    "                    elif target.startswith(\"embeddings/\"):\n",
    "                        return f\"ppt/{target}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "        # Helper to handle normal shapes + grouped shapes\n",
    "        def extract_from_shape(shape):\n",
    "            nonlocal slide_has_content\n",
    "\n",
    "            # If this is a group shape, recurse into its children\n",
    "            if hasattr(shape, \"shapes\") and len(shape.shapes) > 0:\n",
    "                for sub in shape.shapes:\n",
    "                    extract_from_shape(sub)\n",
    "                return\n",
    "\n",
    "            # 1) Standard text frame\n",
    "            if hasattr(shape, \"text_frame\"):\n",
    "                text = shape.text_frame.text.strip()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "                    slide_has_content = True\n",
    "\n",
    "            # 2) Simple text attribute\n",
    "            elif hasattr(shape, \"text\"):\n",
    "                text = shape.text.strip()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "                    slide_has_content = True\n",
    "\n",
    "            # 3) Native PPT tables\n",
    "            if hasattr(shape, \"has_table\") and shape.has_table:\n",
    "                content.append(f\"\\n[Table in Slide {slide_num}]\")\n",
    "                table = shape.table\n",
    "                for row in table.rows:\n",
    "                    cells = [cell.text.strip() for cell in row.cells]\n",
    "                    if any(cells):\n",
    "                        content.append(\" | \".join(cells))\n",
    "                slide_has_content = True\n",
    "\n",
    "            # 4) Embedded Excel / OLE objects: get the CORRECT workbook for THIS shape\n",
    "            if include_embedded_excels:\n",
    "                excel_path = get_embedded_excel_path(shape, slide)\n",
    "                if excel_path and excel_path in embedded_cache:\n",
    "                    sheet_dfs = embedded_cache[excel_path]\n",
    "                    wb_name = os.path.basename(excel_path)\n",
    "                    \n",
    "                    content.append(f\"\\n[Embedded workbook on Slide {slide_num}: {wb_name}]\")\n",
    "                    for sheet_name, df in sheet_dfs.items():\n",
    "                        content.append(f\"\\n--- EMBEDDED SHEET: {sheet_name} ---\")\n",
    "                        content.append(df.head(max_rows).to_csv(index=False).strip())\n",
    "                    slide_has_content = True\n",
    "\n",
    "        # Run the helper for all top-level shapes\n",
    "        for shape in slide.shapes:\n",
    "            extract_from_shape(shape)\n",
    "\n",
    "        # --------- Step 3: SmartArt / diagram text (plain text) ---------\n",
    "        raw_smartart_texts = extract_smartart_texts(slide)\n",
    "\n",
    "        if raw_smartart_texts:\n",
    "            content.append(f\"\\n[SMARTART content on this slide]\")\n",
    "            for t in raw_smartart_texts:\n",
    "                content.append(t)\n",
    "            slide_has_content = True\n",
    "\n",
    "        # --------- Step 4: Slide notes ---------\n",
    "        if slide.has_notes_slide:\n",
    "            try:\n",
    "                notes_text = slide.notes_slide.notes_text_frame.text.strip()\n",
    "                if notes_text:\n",
    "                    content.append(f\"\\n[Notes]: {notes_text}\")\n",
    "                    slide_has_content = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not slide_has_content:\n",
    "            content.append(\"[No text content found on this slide]\")\n",
    "\n",
    "    result = \"\\n\".join(content)\n",
    "\n",
    "    if not result.strip() or result.count(\"[No text content found on this slide]\") == len(prs.slides):\n",
    "        return f\"[PowerPoint file has {len(prs.slides)} slides but no extractable text was found]\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da650bf2-3604-4d19-98a8-e4b845f0a07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Processing all documents in the Azure storage folder\n",
    "\n",
    "This cell scans the configured Azure Data Lake directory and processes every supported file type, including Word, Excel, and PowerPoint documents.  \n",
    "For each file, the corresponding extraction function is applied to read text, tables, or embedded content.  \n",
    "All extracted information is combined into a single text output, which is saved back into the same storage location for later use in the pipeline.  \n",
    "The goal of this step is to unify the content of multiple document formats into one structured text file that can be passed to the AI model in the following stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef03f93b-4307-4cd7-8056-7e7d7f3bd7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files: ['TG3 Project Plan + Establishment v1.2.pptx']\nWrote 23812 bytes.\nCombined content written to: abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/all_content.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define what file types we want to process\n",
    "extensions = [\".pdf\", \".pptx\", \".ppt\", \".docx\", \".doc\", \".xlsx\", \".xls\", \".csv\"]\n",
    "folder_path = INPUT_DIR\n",
    "\n",
    "# List all files in the folder\n",
    "items = dbutils.fs.ls(folder_path)\n",
    "\n",
    "# Filter to only get files with our extensions\n",
    "files = [item.path for item in items if any(item.name.lower().endswith(ext) for ext in extensions)]\n",
    "\n",
    "print(f\"Found {len(files)} files:\", [os.path.basename(f) for f in files])\n",
    "\n",
    "all_content = ''\n",
    "\n",
    "for i, file_path in enumerate(files):\n",
    "    all_content += f\"\\n\\nFile {i+1}: {os.path.basename(file_path)}\\n{'='*60}\\n\"\n",
    "                                                                                                \n",
    "    if file_path.lower().endswith('.docx'):\n",
    "        docx_content = read_docx_text(file_path)\n",
    "        all_content += docx_content + \"\\n\"\n",
    "    \n",
    "    elif file_path.lower().endswith(\".xlsx\"):\n",
    "        sheets = read_xlsx_sheets(file_path)\n",
    "        for sheet_name, df in sheets.items():\n",
    "            all_content += f\"\\n--- SHEET: {sheet_name} ---\\n\"\n",
    "            all_content += df.head().to_csv(index=False) + \"\\n\"\n",
    "\n",
    "    elif file_path.lower().endswith('.pptx'):\n",
    "        # Now this already includes embedded Excels in the correct slide position\n",
    "        pptx_content = read_pptx_text(file_path, include_embedded_excels=True, max_rows=30)\n",
    "        all_content += pptx_content + \"\\n\"\n",
    "\n",
    "    elif file_path.lower().endswith('.ppt'):\n",
    "        all_content += \"[.ppt format not supported by current extractor]\\n\"\n",
    "\n",
    "output_path = f\"{folder_path}/all_content.txt\"\n",
    "dbutils.fs.put(output_path, all_content, overwrite=True)\n",
    "\n",
    "print(f\"Combined content written to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561f4a5b-af34-48d1-94ba-3e0a735622be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\nFile 1: TG3 Project Plan + Establishment v1.2.pptx\n============================================================\n\n--- Slide 1 ---\nTG3\u000BProject Plan + Establishment\u000B\u000BNordic Energy Data Quality Improvement – Phase 1\nAuthor: NordicTech Solutions AB – S&BS Team\nStatus Report Date: 28 Nov 2025\n\n--- Slide 2 ---\nTG3 Project Plan + Establishment\nProject Information:\n\n\n\n\n\n\nRevision History: \n\n\n\n\n\n\n\n\nAppendices:\nAppendix 1: Meter Data Overview\nAppendix 2: Data Cleaning Rules\nNordic Energy Data Quality Improvement – Phase 1\nNovision Criteria: \n<Mark Novision Criteria by changing to Checkmark Bullets>\n\n[Table in Slide 2]\nProject ID | PRJ-NE-DQP-001 / CAT4-E-0785\nProject Portfolio Owner | Emma Lindström\nProject Sponsor | Jonas Hallberg\nProject Manager | Maria Svensson\n+46 70 123 45 67\nmaria.svensson@nordictechsolutions.com\nSAP code | Not applicable\nProject Period | 1 Nov 2025 – 28 Nov 2025\n\n[Table in Slide 2]\nRevision | Description of Change | Resp. | Effective Date\n1.0 | First version | T. Carbin | 2025-11-01 (project start / first approved plan)\n1.1 | Dummy data added to project plan sections | M. Svensson | 2025-11-08 (one week later, still early in the period)\n\n[Table in Slide 2]\n | Criteria | Comment\n | Strategic focus area compliance checked | Aligns with CO₂ reduction target\n | KPIs & Benefit realization detailed | KPIs created in scope section\n | Business Case detailed | Simple internal-cost model\n | Purpose, goals & deliverables detailed | Clear 3-task pilot scope\n | Scope/WBS & delimitations detailed | WBS provided in section 3\n | Requirements detailed | Minimal; access to meter files\n | Consequences, Dependencies, Risks & Handover detailed | Included in dedicated slide\n | Budget detailed and secured | €10,000 internal budget\n | Tender evaluation and final partner(s) selected | Not applicable\n | Solution implementation plan detailed | Timeline prepared\n | Partner contract & SLAs final commitment detailed | Not applicable\n | Organization allocated and approved | Small project team assigned\n | Communication Plan detailed | Weekly email updates\n | Support after Go-live detailed | Maintained by Energy Team\n | Lessons Learned documented | Will be added at TG4\nCurrent revision: 1.1\nStatus Report Date: 28 Nov 2025\n\n--- Slide 3 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nProject Description and Background: \nThe project aims to improve the quality of electricity meter data for three pilot buildings within the Nordic region.\u000BCurrently, the data is stored in separate files with inconsistent formats, missing days, and manual corrections.\u000BThis pilot project focuses on cleaning and standardizing the data to prepare for future reporting and analytics.\n\n\n\n\nPurpose and Benefit Realization: \nThe purpose is to establish reliable and consistent energy consumption data.\u000BThis improves reporting accuracy and supports the company’s CO₂ reduction and energy-efficiency goals.\nBenefits:\nHigher data accuracy\nFaster monthly reporting\nFoundation for future automation and dashboards\nKPIs:\n95% clean and validated data\nOne combined dataset for all 3 buildings\nMonthly usage summary delivered\nNordic Energy Data Quality Improvement – Phase 1\nGoals and Deliverables: \n\nGoals:\nCreate a clean, unified dataset for 3 buildings\nEnable simple monthly reporting\nEnsure data reliability\nDeliverables:\nCleaned dataset\nData validation summary\nOne monthly energy usage report\n\n\n\nResource or Expertise needs:\nProject Manager – coordination and communication\nData Analyst – data cleaning and validation\nIT Technician – access to meter data and source files\nExpertise required:\u000BBasic data analysis, Excel file handling, and understanding of meter data structure.\nStatus Report Date: 28 Nov 2025\n\n--- Slide 4 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nBusiness Impact, Cost, Benefits and ROI: \nThis pilot provides reliable energy data that supports sustainability reporting, cost efficiency, and CO₂ reduction strategy.\n\n\n\n\nCost:\u000BApprox. 75,000 SEK internal labor only.\n\n\n\n\nBenefits:\nMore reliable monthly energy reporting\nReduced manual correction effort (approx. 10–12 hours saved per month)\nFaster availability of validated energy insights for operations\nFewer reporting anomalies and less rework\n\n\n\nROI:\u000BShort-term: Positive due to reduced manual work\nLong-term: High if expanded to additional buildings\nBreak-even: Within the first reporting year\nNordic Energy Data Quality Improvement – Phase 1\nScope/WBS and Delimitations\n\nIn Scope (3 simple tasks)\nExtract meter data from 3 buildings\nClean & standardize the data\nProduce a monthly energy summary report\n\nOut of Scope\nAutomation\nReal-time dashboards\nPredictive analytics\nHardware upgrades\n\nWBS\nData Extraction\nData Cleaning\nSummary Reporting\n\n\nRequirements:\nAccess to energy meter files for all 3 buildings\nSupport from IT Technician for data retrieval\nCompliance with company data handling procedures\nResults should support CO₂ reduction and internal sustainability goals\nNo special health or safety requirements\nStatus Report Date: 28 Nov 2025\n\n--- Slide 5 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nConsequences:\nIf the project is not executed:\nMonthly energy reporting will continue to rely on inconsistent and incomplete data.\nCO₂ reduction and energy-saving initiatives will lack accurate baseline data.\nManual corrections will remain time-consuming and error-prone.\nFuture automation and dashboard development will be delayed.\n\n\n\n\nDependencies:\nAccess to meter data from Building A, B, and C.\nIT support to retrieve and share the raw data files.\nEnergy Team availability to review the cleaned dataset.\nShared folder structure for storing consolidated data.\nThese dependencies are simple and minimal for this pilot phase.\n\n\n\nRisks: \nMissing or corrupted meter data for specific days.\nDelays in obtaining files from IT or building operations.\nInconsistent data formats that require extra cleaning time.\nLimited availability of the Data Analyst during key steps.\nRisk exposure is low due to the small scope of the pilot.\nNordic Energy Data Quality Improvement – Phase 1\nStrategic focus area(s): \n<mark applicable by changing to Checkmark Bullet(s)>\n1. We help our customer to reduce their CO2 footprint\n2. We develop our electricity business to meet new demands\n3. We build new business in flexibility services & integrated energy solutions\n4. We develop IT into a strategic advantage\n5. We engage our employees\n\n\n\nProject Governance and Handover: \nProject Portfolio Owner: Emma Lindström\nHandover Partner: Lars Nyberg (Energy Operations Lead)\nThe cleaned dataset and monthly report will be handed over to the Energy Operations Team at project completion (expected 22–28 Nov 2025).\u000BThey will use this data for monthly reporting and sustainability analyses.\nStatus Report Date: 28 Nov 2025\n\n--- Slide 6 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nAcceptance to include Project in Project Portfolio: \nProject Portfolio Owner PPO\nEmma Lindström\n<date>\t\t\n\nProject Sponsor\nJonas Hallberg\n<date>\t\t\n\nHandover Partner\nLars Nyberg\n<date>\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\n\n--- Slide 7 ---\nTender outcome:\nNo external tendering process was required for this pilot phase.\u000BAll activities will be performed using internal resources from NordicTech Solutions AB, including data extraction, cleaning, and reporting.\u000BNo external vendors, software purchases, or contracts were involved.\n\n\n\n\nImplementation plan:\n\nA simple 4-week implementation plan, aligned with the 3 tasks:\nWeek 1 — Data Extraction (Completed)\n\uD83D\uDCC5 1–7 Nov 2025\nIT Technician retrieves meter data from all 3 buildings\nFiles validated and stored in shared workspace\nWeek 2–3 — Data Cleaning (In Progress)\n\uD83D\uDCC5 8–21 Nov 2025\nCombine and standardize the datasets\nRemove duplicates, fill gaps, validate consumption values\nReview with Energy Team\nWeek 4 — Summary Report Preparation (Not Started)\n\uD83D\uDCC5 22–28 Nov 2025\nCreate monthly usage report\nPrepare clean dataset handover package\nFinal review and closure meeting\n\nInternal stakeholders:\u000BProject Manager, Data Analyst, IT Technician, Energy Team\n\nExternal stakeholders:\u000BNone for this pilot project\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nNordic Energy Data Quality Improvement – Phase 1\nContract, SLA and Payments\nAs this is an internal pilot project, no external contract or SLA is required.\u000BAll work is executed under existing internal agreements and standard working procedures.\n\nPayment Terms:\nTotal estimated cost: 75,000 SEK (internal labor only)\nNo external supplier costs\nNo invoices or payment schedules needed\n\nScope and Quality:\nDeliverables: Clean dataset + monthly summary report\nQuality criteria: 95% validated and complete data\nTesting: Manual validation by Energy Team\nStatus Report Date: 28 Nov 2025\n\n--- Slide 8 ---\nAcceptance of Contract, SLA and Payment Terms: \nProject Portfolio Owner PPO\nEmma Lindström\nApproved on: 1 Nov 2025\n\nProject Sponsor\nJonas Hallberg\nApproved on: 2 Nov 2025\n\nHandover Partner\nLars Nyberg\nApproved on: 2 Nov 2025\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\n\n--- Slide 9 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nRisk Management:\nNordic Energy Data Quality Improvement – Phase 1\n\n[Embedded workbook on Slide 9: Microsoft_Excel_Worksheet.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nNr,Description,Probability (1-5),Consequence (1-5),Value (1-25),Mitigation Action,Responsible,Due date\n1,IT Technician not available to retrieve meter data on time,3,3,9,Book technician time in advance and have a backup contact in IT,IT Technician,End of Week 1\n2,Delay in receiving meter data from one or more buildings,3,3,9,Send early data requests to building staff and follow up regularly,Project Manager,End of Week 1\n3,Missing or corrupted data in meter files,2,4,8,Use fallback sources where possible; document gaps and correct manually,Data Analyst,End of Week 3\n4,Inconsistent data formats between buildings,3,2,6,Define a standard format template and apply common cleaning rules,Data Analyst,End of Week 3\n5,Final report delayed due to review taking longer than planned,2,2,4,Schedule review slot with Energy Team in advance; share draft early,Handover,End of Week 4\nStatus Report Date: 28 Nov 2025\n\n--- Slide 10 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nProject Organization:\nOrganizational structure for the TG3 pilot project (Nov 2025)\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\n\n[SMARTART content on this slide]\nSponsor\nJonas Hallberg\nSteering\nGroup\nElin Bergström\nProject Manager\nMaria Svensson\nIT Project Manager\nDaniel Larsson\nProject\nMember\nSara Holm\nProject\nMember\nMikael Jonsson\nProject Member (Data Analyst)\nAnna Karlsson\nProject Member (Energy Operations Rep)\nLars Nyberg\nProject Member (IT Support Technician)\nErik Johansson\nSponsor\nJonas Hallberg\nProject\nMember\nSara Holm\nProject Manager\nMaria Svensson\nIT Project Manager\nDaniel Larsson\nProject Member (Data Analyst)\nAnna Karlsson\nProject\nMember\nMikael Jonsson\nProject Member (Energy Operations Rep)\nLars Nyberg\nProject Member (IT Support Technician)\nErik Johansson\nSteering\nGroup\nElin Bergström\n\n--- Slide 11 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nWork Breakdown Structure - WBS:\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\nTimeline:\nWP1.x tasks take place in Week 1 (1–7 Nov)\nWP2.x tasks take place in Weeks 2–3 (8–21 Nov)\nWP3.x tasks take place in Week 4 (22–28 Nov)\n\n[SMARTART content on this slide]\nScope\nArea 1: Data Extraction\nArea 2: Data Cleaning & Standardization\nArea 3: Repoirting & Handover\nWP1.1 – Retrieve meter data from Building A\nWP1.2 – Retrieve meter data from Building B\nWP2.1 – Combine datasets into a unified format\nWP2.2 – Remove duplicates and fill misssing values\nWP3.1 – Create monthly energy summary report\nWP3.2 – Prepare clean dataset package\nWP1.3 – Retrieve meter data from Building C\nWP2.3 – Validate consumption values with Energy Team\nWP3.3 – Handover to Energy Operations Team\nScope\nArea 1: Data Extraction\nWP1.1 – Retrieve meter data from Building A\nWP1.2 – Retrieve meter data from Building B\nWP1.3 – Retrieve meter data from Building C\nArea 2: Data Cleaning & Standardization\nWP2.1 – Combine datasets into a unified format\nWP2.2 – Remove duplicates and fill misssing values\nWP2.3 – Validate consumption values with Energy Team\nArea 3: Repoirting & Handover\nWP3.1 – Create monthly energy summary report\nWP3.2 – Prepare clean dataset package\nWP3.3 – Handover to Energy Operations Team\n\n--- Slide 12 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nNetwork Diagram\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\nTimeline:\nWP1.x tasks take place in Week 1 (1–7 Nov)\nWP2.x tasks take place in Weeks 2–3 (8–21 Nov)\nWP3.x tasks take place in Week 4 (22–28 Nov)\n\n[SMARTART content on this slide]\nStart\nWP1.1 – Retrieve meter data from Building A\nWP1.2 – Retrieve meter data from Building B\nWP1.3 – Retrieve meter data from C\nWP2.1 – Combine datasets into unified format\nWP2.2 – Clean missing/duplicate values\nWP2.3 – Validate cleaned data with Energy Team\nWP3.1 - Create monthly energy summary report\nWP3.2 - Prepare clean dataset package\nWP3.3 – Handover to Energy Operations\nStop\nStart\nWP1.1 – Retrieve meter data from Building A\nWP1.2 – Retrieve meter data from Building B\nWP2.1 – Combine datasets into unified format\nWP2.2 – Clean missing/duplicate values\nWP2.3 – Validate cleaned data with Energy Team\nWP3.1 - Create monthly energy summary report\nWP3.3 – Handover to Energy Operations\nStop\nWP3.2 - Prepare clean dataset package\nWP1.3 – Retrieve meter data from C\n\n--- Slide 13 ---\n\n[Embedded workbook on Slide 13: Microsoft_Excel_Worksheet1.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nNr,Activity,Description,Start (Date),Stop (Date),Duration (Hours/Days),Dependency\n1,Data Extraction,\"Retrieve meter data from Buildings A, B, C and check that files are complete\",2025-01-11 00:00:00,2025-07-11 00:00:00,7,Start\n2,Data Cleaning & Standardization,\"Combine datasets, remove missing/duplicate values, standardize columns\",2025-08-11 00:00:00,21/11/2025,14,Activity 1\n3,Reporting & Handover,Validate cleaned dataset with Energy Team,15/11/2025,21/11/2025,7,Activity 2 (Overlaps allowed)\n3a,Reporting – Summary Report,Create monthly summary report,22/11/2025,24/11/2025,3,Activity 2 & 3\n3b,Reporting – Dataset Package,Prepare clean dataset package,22/11/2025,26/11/2025,5,Activity 2 & 3\n3c,Final Handover,Final handover to Energy Operations,27/11/2025,28/11/2025,2,Activity 4 & 5\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nActivity List:\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\n\n--- Slide 14 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nGantt Chart:\nNordic Energy Data Quality Improvement – Phase 1\n\n[Table in Slide 14]\nActivity | Week 1 (Nov 1–7) | Week 2 (Nov 8–14) | Week 3 (Nov 15–21) | Week 4 (Jun 22–28)\nData Extraction | (7 Days) |  |  | \nData Cleaning & Standardization |  | (7 Days) | (7 Days) | \nReporting & Handover |  |  |  | (7 Days)\nStatus Report Date: 28 Nov 2025\n\n--- Slide 15 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nSWOT Analysis:\nNordic Energy Data Quality Improvement – Phase 1\nSTRENGTHS (Internal, Helpful)\nClear and limited projects scope (only 3 tasks)\nUses internal resource only – no external dependencies\nFast implementation (4 weeks total)\nStrong support from Energy Operations Team\nImproves data reliability immediately\nWEAKNESSES (Internal, Harmful)\nManual data cleaning required\nSmall project team may have competing priorities\nDepends on timely input from IT Technician and Energy Team\nLimited to 3 pilot buildings (not full company portfolio)\nOPPORTUNITIES (External, Helpful)\nCan extend to real-time monitoring dashboards in future phases\nSupports CO2 reduction and sustainability reporting needs\nEnables data-driven energy savings across more buildings\nPotential to automate cleaning and reporting in future phases\nCreates foundation for smarter energy analytics\nTHREATS (External, Harmful)\nDelays from building operations or IT access issues\nFuture meter hardware upgrades may change data formats\nUnexpected data quality issues may increase workload\nOther corporate priorities may limit resource availability\nHELPFUL\nHARMFUL\nEXTERNAL\nINTERNAL\nStatus Report Date: 28 Nov 2025\nSWOT Analysis:\n\n--- Slide 16 ---\n\n[Embedded workbook on Slide 16: Microsoft_Excel_Worksheet2.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nStakeholder,Description of interest / impact / or need of information,Commitment\nJonas Hallberg (Sponsor),Ensures project funding and approves final deliverables. Needs regular status updates.,1\nMaria Svensson (Project Manager),\"Coordinates tasks, ensures progress, manages risks, needs full visibility.\",1\nDaniel Larsson (IT Project Manager),Ensures technical access and supports IT coordination.,2\nAnna Karlsson (Data Analyst),\"Performs data cleaning and standardization, critical for task completion.\",2\nLars Nyberg (Energy Operations Lead),\"Reviews cleaned dataset, receives final deliverables.\",3\nErik Johansson (IT Technician),Retrieves meter files and ensures technical access.,3\nEnergy Team (Operations),Uses the final dataset for reporting; needs final update only.,4\n\"Building Staff (A, B, C)\",Provides meter access if needed. Only requires minimal notifications.,5\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nStakeholder mapping:\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\n\n--- Slide 17 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nCommunication Plan:\nNordic Energy Data Quality Improvement – Phase 1\n\n[Embedded workbook on Slide 17: Microsoft_Excel_Worksheet3.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nNr,How? (Meeting),When? (Frequency),What? (Agenda),Who? (Participants),Where? (Location)\n1,Project Team Meeting,\"Weekly, 30 min\",\"Progress updates, upcoming tasks, risks, data issues\",\"Project Manager, Data Analyst, IT Project Manager, IT Technician\",Teams\n2,Steering Group Update,\"Bi-weekly, 20 min\",\"Status, risks, decisions needed\",\"Project Manager, Sponsor, Steering Group\",Teams\n3,Data Review Session,At completion of data cleaning,\"Review cleaned dataset, validate results, confirm correctness\",\"Project Manager, Data Analyst, Energy Operations Lead\",On site or Teams\n4,Final Handover Meeting,End of Week 4,\"Present summary report, deliver clean dataset, confirm transition\",\"Project Manager, Energy Operations Team, Sponsor\",On site or Teams\nStatus Report Date: 28 Nov 2025\n\n--- Slide 18 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nHandover Management:\nThe Project Manager hands over the cleaned dataset and summary report to the Handover Manager in the Operating Unit once TG4 is approved.\u000BThe Handover Manager is responsible for ensuring that the Energy Operations team receives all updated documentation and that the data is ready for ongoing monthly reporting.\nThe Operating Unit Manager must ensure resources are available for the transition and appoint a Super User who will support daily operations after the project ends. The Super User will act as the local contact for any questions regarding the cleaned dataset and reporting process.\nThe Sponsor approves the final delivery and ensures that expected benefits (improved data quality, smoother reporting, reduced manual corrections) are realized over time.\nNordic Energy Data Quality Improvement – Phase 1\nProject Organization\nOperating Unit\nMaintenance\nHandover Manager\nSuper User\nManager\nHenrik Olofsson\nSofia Bergström\nJohan Wiklund\nStatus Report Date: 28 Nov 2025\n\n[SMARTART content on this slide]\nSponsor\n[Jonas Hallberg]\nProject Manager\nMaria Svensson\nIT Project Manager\nDaniel Larsson]\nProject Member (Energy Ops)\nLars Nyberg\nProject Member\n(Data Analyst)\nAnna Karlsson\nSteering Group\nEva Lundström\nProject Member (IT Technician)\nErik Johansson\nSponsor\n[Jonas Hallberg]\nSteering Group\nEva Lundström\nProject Manager\nMaria Svensson\nIT Project Manager\nDaniel Larsson]\nProject Member\n(Data Analyst)\nAnna Karlsson\nProject Member (Energy Ops)\nLars Nyberg\nProject Member (IT Technician)\nErik Johansson\n\n--- Slide 19 ---\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nLessons Learned:\nNordic Energy Data Quality Improvement – Phase 1\n\n[Embedded workbook on Slide 19: Microsoft_Excel_Worksheet4.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nNr,Area,What went well?,What could have been done better?\n1,Project Planning,\"Scope was clear, timeline was easy to follow, and responsibilities were well-defined.\",Earlier alignment with the Operating Unit could have reduced last-minute questions.\n2,Data Extraction,\"Meter data from Buildings A, B, and C was retrieved smoothly with good support from IT.\",Some file structures were inconsistent; a standard format request could have saved time.\n3,Data Cleaning,Cleaning and standardization tools worked efficiently; few technical issues occurred.,More automation could reduce manual steps in future phases.\n4,Collaboration,\"Communication between PM, IT, and Energy Ops was effective and timely.\",Availability of IT Technician was limited on some days; early scheduling could help.\n5,Reporting & Handover,Final dataset and summary report were delivered on time and accepted without major changes.,More visual dashboards could be added to make insights clearer for stakeholders.\nStatus Report Date: 28 Nov 2025\n\n--- Slide 20 ---\n\n[Embedded workbook on Slide 20: Microsoft_Excel_Worksheet5.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nNr,Activity,Description,Deadline,Responsible,Status\n1,Dashboard Prototype,Create a simple Power BI dashboard using the cleaned dataset (Phase 2 item),15/01/2026,Data Analyst (Anna Karlsson),Not started\n2,Automation Proposal,Prepare a short proposal for automating monthly data cleaning (future improvement),2026-12-01 00:00:00,Project Manager (Maria Svensson),In progress\n3,Data Dictionary,\"Document each column, units, and data rules for long-term usage\",2026-10-01 00:00:00,Energy Ops Lead (Lars Nyberg),Not started\nTG3 Project Plan + Establishment\u000BProject Period: 1 Nov 2025 – 28 Nov 2025\nRest Activities:\nNordic Energy Data Quality Improvement – Phase 1\nStatus Report Date: 28 Nov 2025\n\n--- Slide 21 ---\n\n[Embedded workbook on Slide 21: Microsoft_Excel_Worksheet6.xlsx]\n\n--- EMBEDDED SHEET: Sheet1 ---\nNr,Cost Category,Planned (SEK),Actual (SEK),Difference,Status\n1,Personnel Cost,60000,100000,-40000,Not started\n2,IT Support / Access,10000,11000,-1000,In progress\n3,Reporting & Documentation,5000,4000,1000,Not started\n4,Total,75000,115000,-40000,\nTG3 Project Plan + Establishment\nFinancial Performance\nNordic Energy Data Quality Improvement – Phase 1\nBenefits / Value (Financial + Operational)\nValue Delivered\nReduced manual data cleaning time by ~12 hours/month\nImproved data quality → fewer reporting errors\nPotential long-term savings through automation\nFaster monthly reporting process\n.\nROI Snapshot\nROI Assessment\nShort-term ROI: Moderate, due to small pilot scope\nLong-term ROI: High, if extended to more buildings and \nBreak-even: Achievable within the first full reporting cycle if scaled\nStatus Report Date: 28 Nov 2025\n\n"
     ]
    }
   ],
   "source": [
    "if True: # set to True to test\n",
    "    output_path = f\"{INPUT_DIR}/all_content.txt\"\n",
    "    data = dbutils.fs.head(output_path, 100000000)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46697db6-bfa6-48ad-836c-c65f7b6a407b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a project management data analyst extracting statuses from project documents. I will provide text extracted from a project management document, and you will extract status information from it.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Only extract information explicitly present in the provided document.\n",
    "- Do not make assumptions, estimates, or guesses.\n",
    "- Do not invent or hallucinate values.\n",
    "- If information is missing or unclear, do not generate a KPI for it.\n",
    "- Keep wording and numbers as close as possible to the original source.\n",
    "\n",
    "Pay attention to Status Report Date. Analyze the project and return:\n",
    "\n",
    "1. A financial status score between 0 and 1, and a concise explanation of why\n",
    "2. A timing status score between 0 and 1, and a concise explanation that explicitly uses the Status Report Date, compares the planned schedule (as described in the implementation plan in the document) with the actual progress reported, and clearly states whether the project is on track, slightly behind, significantly behind, or ahead of schedule, including a short justification for the chosen score.\n",
    "3. The resource status must evaluate whether the project has the required roles, people, and support available as described in the document. The explanation should compare planned resource availability versus actual availability or constraints mentioned in the text, and state whether resources are sufficient, partially insufficient, or blocking the progress and mention based on which risks the value estimated. The score (0–1) should reflect this assessment.\n",
    "4. A task status score between 0 and 1, and a concise explanation of why\n",
    "5. An overall status score between 0 and 1, and a concise explanation of why\n",
    "\n",
    "Respond only in valid JSON using this structure (no extra text, comments, or formatting outside the JSON):\n",
    "\n",
    "{{\n",
    "  \"Financial\": {{\n",
    "    \"StatusDescription\": \"<short explanation of financial status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "  \"Timing\": {{\n",
    "    \"StatusDescription\": \"<short explanation of timing status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "  \"Resources\": {{\n",
    "    \"StatusDescription\": \"<short explanation of resource status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "\n",
    "    \"Tasks\": {{\n",
    "    \"Tasks Description In Project\": \"<short explanation of overall status with bulletpoints>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }},\n",
    "\n",
    "  \"Overall\": {{\n",
    "    \"StatusDescription\": \"<short explanation of overall status>\",\n",
    "    \"Status\": <number between 0 and 1>\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Here is the project management document’s extracted text:\n",
    "{all_content}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daeea6c-13b7-4803-815e-5aaddf6aa8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DEPLOYMENT = \"gpt-4o-sweden\"\n",
    "# DEPLOYMENT = \"gpt-5-chat-latest\"\n",
    "response = client.chat.completions.create(\n",
    "    model=DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a project management analyst.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed17e0f-8561-4d2e-a50c-57614b1e8da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\ngpt-4\ngpt-3.5-turbo\ngpt-5.1-codex-max\ngpt-5.1-2025-11-13\ngpt-5.1\ngpt-5.1-codex\ngpt-5.1-codex-mini\ndavinci-002\nbabbage-002\ngpt-3.5-turbo-instruct\ngpt-3.5-turbo-instruct-0914\ndall-e-3\ndall-e-2\ngpt-4-1106-preview\ngpt-3.5-turbo-1106\ntts-1-hd\ntts-1-1106\ntts-1-hd-1106\ntext-embedding-3-small\ntext-embedding-3-large\ngpt-4-0125-preview\ngpt-4-turbo-preview\ngpt-3.5-turbo-0125\ngpt-4-turbo\ngpt-4-turbo-2024-04-09\ngpt-4o\ngpt-4o-2024-05-13\ngpt-4o-mini-2024-07-18\ngpt-4o-mini\ngpt-4o-2024-08-06\nchatgpt-4o-latest\ngpt-4o-audio-preview\ngpt-4o-realtime-preview\nomni-moderation-latest\nomni-moderation-2024-09-26\ngpt-4o-realtime-preview-2024-12-17\ngpt-4o-audio-preview-2024-12-17\ngpt-4o-mini-realtime-preview-2024-12-17\ngpt-4o-mini-audio-preview-2024-12-17\no1-2024-12-17\no1\ngpt-4o-mini-realtime-preview\ngpt-4o-mini-audio-preview\no3-mini\no3-mini-2025-01-31\ngpt-4o-2024-11-20\ngpt-4o-search-preview-2025-03-11\ngpt-4o-search-preview\ngpt-4o-mini-search-preview-2025-03-11\ngpt-4o-mini-search-preview\ngpt-4o-transcribe\ngpt-4o-mini-transcribe\no1-pro-2025-03-19\no1-pro\ngpt-4o-mini-tts\no3-2025-04-16\no4-mini-2025-04-16\no3\no4-mini\ngpt-4.1-2025-04-14\ngpt-4.1\ngpt-4.1-mini-2025-04-14\ngpt-4.1-mini\ngpt-4.1-nano-2025-04-14\ngpt-4.1-nano\ngpt-image-1\ncodex-mini-latest\no3-pro\ngpt-4o-realtime-preview-2025-06-03\ngpt-4o-audio-preview-2025-06-03\no3-pro-2025-06-10\no4-mini-deep-research\no3-deep-research\ngpt-4o-transcribe-diarize\no3-deep-research-2025-06-26\no4-mini-deep-research-2025-06-26\ngpt-5-chat-latest\ngpt-5-2025-08-07\ngpt-5\ngpt-5-mini-2025-08-07\ngpt-5-mini\ngpt-5-nano-2025-08-07\ngpt-5-nano\ngpt-audio-2025-08-28\ngpt-realtime\ngpt-realtime-2025-08-28\ngpt-audio\ngpt-5-codex\ngpt-image-1-mini\ngpt-5-pro-2025-10-06\ngpt-5-pro\ngpt-audio-mini\ngpt-audio-mini-2025-10-06\ngpt-5-search-api\ngpt-realtime-mini\ngpt-realtime-mini-2025-10-06\nsora-2\nsora-2-pro\ngpt-5-search-api-2025-10-14\ngpt-5.1-chat-latest\ngpt-3.5-turbo-16k\ntts-1\nwhisper-1\ntext-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fefcb5-ba48-4040-b2e0-3fcb9805365f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1612 bytes.\nOutput JSON saved to: abfss://itera-extraction-project@iterastorerm.dfs.core.windows.net/output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "raw_output = response.choices[0].message.content.strip()\n",
    "# if the model returens markdown code blocks, remove them\n",
    "if raw_output.startswith(\"```\"):\n",
    "    raw_output = re.sub(r'^```json?\\s*', '', raw_output)\n",
    "    raw_output = re.sub(r'```\\s*$', '', raw_output)    \n",
    "\n",
    "output = json.loads(raw_output)\n",
    "output_path = f\"{INPUT_DIR}/output.json\"\n",
    "output_json = json.dumps(output, indent=4, ensure_ascii=False)\n",
    "dbutils.fs.put(output_path, output_json, overwrite=True)\n",
    "print(f\"Output JSON saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588f00f5-ff95-4d66-a7f7-80ffe0a7fb07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'Financial': {'StatusDescription': 'Actual cost 115,000\\u202fSEK vs planned 75,000\\u202fSEK (≈40,000\\u202fSEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.',\n",
       "  'Status': 0.4},\n",
       " 'Timing': {'StatusDescription': 'Status Report Date 28\\u202fNov\\u202f2025 matches planned project end (1–28\\u202fNov\\u202f2025). Implementation plan shows Week\\u202f1 Data Extraction completed, Weeks\\u202f2–3 Data Cleaning in progress, Week\\u202f4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.',\n",
       "  'Status': 0.7},\n",
       " 'Resources': {'StatusDescription': 'Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.',\n",
       "  'Status': 0.8},\n",
       " 'Tasks': {'Tasks Description In Project': '• Data Extraction – Completed\\n• Data Cleaning & Standardization – In Progress\\n• Summary Report Preparation – Not Started (as of 28\\u202fNov\\u202f2025)\\n• Handover – Planned end of Week\\u202f4',\n",
       "  'Status': 0.6},\n",
       " 'Overall': {'StatusDescription': 'Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.',\n",
       "  'Status': 0.6}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_str = dbutils.fs.head(f\"{INPUT_DIR}/output.json\", 100000)\n",
    "raw = json.loads(json_str)\n",
    "\n",
    "display(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4630b5-900f-4d3a-8f4f-9b91cac3088b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "rows = [\n",
    "    Row(\n",
    "        kpi_name=\"Financial\",\n",
    "        status_description=raw[\"Financial\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Financial\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Timing\",\n",
    "        status_description=raw[\"Timing\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Timing\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Resources\",\n",
    "        status_description=raw[\"Resources\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Resources\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Overall\",\n",
    "        status_description=raw[\"Overall\"][\"StatusDescription\"],\n",
    "        status=float(raw[\"Overall\"][\"Status\"])\n",
    "    ),\n",
    "    Row(\n",
    "        kpi_name=\"Tasks\",\n",
    "        status_description=raw[\"Tasks\"][\"Tasks Description In Project\"],\n",
    "        status=float(raw[\"Tasks\"][\"Status\"])\n",
    "    )\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "df = df.withColumn(\"ingested_at\", F.current_timestamp())\n",
    "\n",
    "# Write to a new Delta table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"project_status_kpi_long\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93295bf7-148d-439e-bae6-2b3cc9f782a3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763645814203}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>kpi_name</th><th>status_description</th><th>status</th><th>ingested_at</th></tr></thead><tbody><tr><td>Tasks</td><td>• Data Extraction – Completed\n",
       "• Data Cleaning & Standardization – In Progress\n",
       "• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n",
       "• Handover – Planned end of Week 4</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Resources</td><td>Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.</td><td>0.8</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Timing</td><td>Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.</td><td>0.7</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Overall</td><td>Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Financial</td><td>Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.</td><td>0.4</td><td>2025-12-05T15:02:30.63Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Tasks",
         "• Data Extraction – Completed\n• Data Cleaning & Standardization – In Progress\n• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n• Handover – Planned end of Week 4",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Resources",
         "Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.",
         0.8,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Timing",
         "Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.",
         0.7,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Overall",
         "Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Financial",
         "Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.",
         0.4,
         "2025-12-05T15:02:30.63Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 25
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "kpi_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ingested_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM project_status_kpi_long\n",
    "ORDER BY ingested_at DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425cce71-eba5-4869-a59e-f36970665da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>kpi_name</th><th>status_description</th><th>status</th><th>ingested_at</th></tr></thead><tbody><tr><td>Timing</td><td>Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.</td><td>0.7</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Resources</td><td>Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.</td><td>0.8</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Overall</td><td>Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Tasks</td><td>• Data Extraction – Completed\n",
       "• Data Cleaning & Standardization – In Progress\n",
       "• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n",
       "• Handover – Planned end of Week 4</td><td>0.6</td><td>2025-12-05T15:02:30.63Z</td></tr><tr><td>Financial</td><td>Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.</td><td>0.4</td><td>2025-12-05T15:02:30.63Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Timing",
         "Status Report Date 28 Nov 2025 matches planned project end (1–28 Nov 2025). Implementation plan shows Week 1 Data Extraction completed, Weeks 2–3 Data Cleaning in progress, Week 4 Summary Report not started. At report date, final deliverables should be complete but are still pending, indicating the project is slightly behind schedule.",
         0.7,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Resources",
         "Planned roles (Project Manager, Data Analyst, IT Technician, Energy Team) are assigned. Risks mention limited availability of Data Analyst and IT Technician, but overall resource exposure is low. Resources are partially insufficient due to occasional availability constraints.",
         0.8,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Overall",
         "Project largely on track but slightly behind schedule and over budget. Core resources available though with minor constraints. Deliverables nearing completion but final reporting pending at status date.",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Tasks",
         "• Data Extraction – Completed\n• Data Cleaning & Standardization – In Progress\n• Summary Report Preparation – Not Started (as of 28 Nov 2025)\n• Handover – Planned end of Week 4",
         0.6,
         "2025-12-05T15:02:30.63Z"
        ],
        [
         "Financial",
         "Actual cost 115,000 SEK vs planned 75,000 SEK (≈40,000 SEK over). Internal labor only, no external costs. Budget exceeded but ROI remains positive for pilot scope.",
         0.4,
         "2025-12-05T15:02:30.63Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 26
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "kpi_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ingested_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES;\n",
    "SELECT * FROM project_status_kpi_long;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef9bb4a2-bfa6-48ea-aaac-4a884fbd32da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8866569949184394,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_extraction_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}